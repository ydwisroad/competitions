{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-06-16T13:27:22.582628Z",
     "iopub.status.busy": "2021-06-16T13:27:22.582224Z",
     "iopub.status.idle": "2021-06-16T13:27:30.601496Z",
     "shell.execute_reply": "2021-06-16T13:27:30.600630Z",
     "shell.execute_reply.started": "2021-06-16T13:27:22.582544Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import nn \n",
    "import torch.nn.functional as F\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import pytorch_lightning as pl \n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import model_selection\n",
    "import transformers\n",
    "from transformers import get_linear_schedule_with_warmup, AdamW\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-05T04:07:59.044243Z",
     "iopub.status.busy": "2021-06-05T04:07:59.043934Z",
     "iopub.status.idle": "2021-06-05T04:07:59.050975Z",
     "shell.execute_reply": "2021-06-05T04:07:59.050177Z",
     "shell.execute_reply.started": "2021-06-05T04:07:59.044208Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data and Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-05T04:07:59.053456Z",
     "iopub.status.busy": "2021-06-05T04:07:59.053016Z",
     "iopub.status.idle": "2021-06-05T04:07:59.130644Z",
     "shell.execute_reply": "2021-06-05T04:07:59.129435Z",
     "shell.execute_reply.started": "2021-06-05T04:07:59.05342Z"
    }
   },
   "outputs": [],
   "source": [
    "#taking only the id,excerpt,target,standard_error\n",
    "df = pd.read_csv(\"../input/commonlitreadabilityprize/train.csv\",usecols=[\"id\",\"excerpt\",\"target\",\"standard_error\"])\n",
    "test_df = pd.read_csv(\"../input/commonlitreadabilityprize/test.csv\",usecols=[\"id\",\"excerpt\"])\n",
    "print(\"train shape\",df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-05T04:07:59.132731Z",
     "iopub.status.busy": "2021-06-05T04:07:59.132289Z",
     "iopub.status.idle": "2021-06-05T04:07:59.305092Z",
     "shell.execute_reply": "2021-06-05T04:07:59.304343Z",
     "shell.execute_reply.started": "2021-06-05T04:07:59.132688Z"
    }
   },
   "outputs": [],
   "source": [
    "# remove outlier\n",
    "df = df[df['standard_error']!=0]\n",
    "plt.scatter(df['target'], df['standard_error'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-05T04:07:59.308121Z",
     "iopub.status.busy": "2021-06-05T04:07:59.307878Z",
     "iopub.status.idle": "2021-06-05T04:07:59.326001Z",
     "shell.execute_reply": "2021-06-05T04:07:59.325116Z",
     "shell.execute_reply.started": "2021-06-05T04:07:59.308097Z"
    }
   },
   "outputs": [],
   "source": [
    "#remove \\n and replace \\'s with 'sfrom the text\n",
    "def prep_text(text_df):\n",
    "    text_df = text_df.str.replace(\"\\n\",\"\",regex=False) \n",
    "    return text_df.str.replace(\"\\'s\",r\"s\",regex=True).values\n",
    "df[\"excerpt\"] = prep_text(df[\"excerpt\"])\n",
    "test_df[\"excerpt\"] = prep_text(test_df[\"excerpt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-05T04:07:59.327713Z",
     "iopub.status.busy": "2021-06-05T04:07:59.327316Z",
     "iopub.status.idle": "2021-06-05T04:07:59.367182Z",
     "shell.execute_reply": "2021-06-05T04:07:59.366373Z",
     "shell.execute_reply.started": "2021-06-05T04:07:59.327676Z"
    }
   },
   "outputs": [],
   "source": [
    "max_words = df[\"excerpt\"].apply(lambda x: len(x.split())).max()\n",
    "print(\"maximum words in instance:\",max_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RoBERTa test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-05T04:07:59.368738Z",
     "iopub.status.busy": "2021-06-05T04:07:59.368261Z",
     "iopub.status.idle": "2021-06-05T04:07:59.991963Z",
     "shell.execute_reply": "2021-06-05T04:07:59.991116Z",
     "shell.execute_reply.started": "2021-06-05T04:07:59.368702Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"../input/roberta-transformers-pytorch/roberta-large/\")\n",
    "out = tokenizer(df[\"excerpt\"][0])\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-05T04:07:59.995253Z",
     "iopub.status.busy": "2021-06-05T04:07:59.9949Z",
     "iopub.status.idle": "2021-06-05T04:08:00.149896Z",
     "shell.execute_reply": "2021-06-05T04:08:00.149125Z",
     "shell.execute_reply.started": "2021-06-05T04:07:59.995215Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"../input/roberta-transformers-pytorch/roberta-large/\")\n",
    "out = tokenizer(df[\"excerpt\"][0])\n",
    "print(out)\n",
    "print(tokenizer.encode(df[\"excerpt\"][0]))\n",
    "print(tokenizer.build_inputs_with_special_tokens(tokenizer.encode(df[\"excerpt\"][0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-05T04:08:00.151988Z",
     "iopub.status.busy": "2021-06-05T04:08:00.151673Z",
     "iopub.status.idle": "2021-06-05T04:08:00.198795Z",
     "shell.execute_reply": "2021-06-05T04:08:00.198004Z",
     "shell.execute_reply.started": "2021-06-05T04:08:00.151953Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_folds(data, num_splits):\n",
    "    # we create a new column called kfold and fill it with -1\n",
    "    data[\"kfold\"] = -1\n",
    "    \n",
    "    # the next step is to randomize the rows of the data\n",
    "    data = data.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    # calculate number of bins by Sturge's rule\n",
    "    # I take the floor of the value, you can also\n",
    "    # just round it\n",
    "    num_bins = int(np.floor(1 + np.log2(len(data))))\n",
    "    \n",
    "    # bin targets\n",
    "    data.loc[:, \"bins\"] = pd.cut(\n",
    "        data[\"target\"], bins=num_bins, labels=False\n",
    "    )\n",
    "    \n",
    "    # initiate the kfold class from model_selection module\n",
    "    kf = model_selection.StratifiedKFold(n_splits=num_splits)\n",
    "    \n",
    "    # fill the new kfold column\n",
    "    # note that, instead of targets, we use bins!\n",
    "    for f, (t_, v_) in enumerate(kf.split(X=data, y=data.bins.values)):\n",
    "        data.loc[v_, 'kfold'] = f\n",
    "    \n",
    "    # drop the bins column\n",
    "    data = data.drop(\"bins\", axis=1)\n",
    "\n",
    "    # return dataframe with folds\n",
    "    return data\n",
    "\n",
    "# read training data\n",
    "df = pd.read_csv(\"../input/commonlitreadabilityprize/train.csv\")\n",
    "df = df[df['standard_error']!=0]\n",
    "\n",
    "# create folds\n",
    "df = create_folds(df, num_splits=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RoBERTa Model and Training Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-05T04:08:00.200397Z",
     "iopub.status.busy": "2021-06-05T04:08:00.200059Z",
     "iopub.status.idle": "2021-06-05T04:08:00.30912Z",
     "shell.execute_reply": "2021-06-05T04:08:00.308266Z",
     "shell.execute_reply.started": "2021-06-05T04:08:00.200363Z"
    }
   },
   "outputs": [],
   "source": [
    "m1 = nn.Conv1d(205, 128, kernel_size=3, stride=1, padding=2)\n",
    "m2 = nn.MaxPool1d(3)\n",
    "m3 = nn.Conv1d(128, 64, kernel_size=3, stride=1, padding=2)\n",
    "m4 = nn.MaxPool1d(3)\n",
    "m5 = nn.Conv1d(64, 1, kernel_size=3, stride=1, padding=2)\n",
    "m6 = nn.MaxPool1d(3)\n",
    "input = torch.randn(8, 205, 768)\n",
    "output = m1(input)\n",
    "output = m2(output)\n",
    "print(output.size())\n",
    "output = m3(output)\n",
    "output = m4(output)\n",
    "print(output.size())\n",
    "output = m5(output)\n",
    "output = m6(output)\n",
    "print(output.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-05T04:08:00.310735Z",
     "iopub.status.busy": "2021-06-05T04:08:00.310381Z",
     "iopub.status.idle": "2021-06-05T04:08:00.316816Z",
     "shell.execute_reply": "2021-06-05T04:08:00.315836Z",
     "shell.execute_reply.started": "2021-06-05T04:08:00.3107Z"
    }
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "EPOCHS = 100\n",
    "NUM_TRAIN_STEPS = int((df.shape[0]/BATCH_SIZE)*EPOCHS)\n",
    "NUM_WARMUP_STEPS = 0\n",
    "FOLDS = df.kfold.unique()\n",
    "NUM_FOLDS = df.kfold.nunique() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-05T04:08:00.318506Z",
     "iopub.status.busy": "2021-06-05T04:08:00.318112Z",
     "iopub.status.idle": "2021-06-05T04:08:00.326081Z",
     "shell.execute_reply": "2021-06-05T04:08:00.325228Z",
     "shell.execute_reply.started": "2021-06-05T04:08:00.31846Z"
    }
   },
   "outputs": [],
   "source": [
    "class RMSELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-8\n",
    "        \n",
    "    def forward(self,output,target):\n",
    "        return torch.sqrt(F.mse_loss(output,target)+self.eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-05T04:08:00.329767Z",
     "iopub.status.busy": "2021-06-05T04:08:00.327135Z",
     "iopub.status.idle": "2021-06-05T04:08:00.345776Z",
     "shell.execute_reply": "2021-06-05T04:08:00.344937Z",
     "shell.execute_reply.started": "2021-06-05T04:08:00.329737Z"
    }
   },
   "outputs": [],
   "source": [
    "class BertModel(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = transformers.AutoModel.from_pretrained(\"../input/roberta-transformers-pytorch/roberta-base/\")\n",
    "        #self.model = transformers.AutoModel.from_pretrained(\"../input/huggingface-bert/bert-large-uncased\")\n",
    "        #self.model = transformers.AutoModel.from_pretrained(\"../input/roberta-transformers-pytorch/roberta-large\")\n",
    "        self.drop = nn.Dropout(0.3)\n",
    "        self.fc = nn.Linear(768,2)  # output to 2 dimensions, targets and errors\n",
    "        \n",
    "        # convolutional layer\n",
    "        self.conv1 = nn.Conv1d(205, 128, kernel_size=3, stride=1, padding=3)\n",
    "        self.conv2 = nn.Conv1d(128, 64, kernel_size=3, stride=1, padding=3)\n",
    "        self.conv3 = nn.Conv1d(64, 1, kernel_size=3, stride=1, padding=3)\n",
    "        self.ReLU = nn.ReLU()\n",
    "        self.pool = nn.MaxPool1d(3)\n",
    "        #self.fc_conv = nn.Linear(257,2)\n",
    "        self.fc_conv = nn.Linear(30,2)\n",
    "        \n",
    "        #self.fc = nn.Linear(1024,2)\n",
    "    \n",
    "    def forward(self,inputs):\n",
    "        out = self.model(**inputs) # output from BERT model\n",
    "        last_hiddens = out[0]\n",
    "        #print(last_hiddens.size())\n",
    "        #out = self.drop(last_hiddens[:,0,:].squeeze(1))\n",
    "        out = self.conv1(last_hiddens)\n",
    "        out = self.ReLU(out)\n",
    "        out = self.pool(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.ReLU(out)\n",
    "        out = self.pool(out)\n",
    "        out = self.conv3(out)\n",
    "        out = self.ReLU(out)\n",
    "        out = self.pool(out)\n",
    "        #print(out.size())\n",
    "        return self.fc_conv(out)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        no_decay = ['bias', 'LayerNorm.weight']\n",
    "        optimizer_grouped_parameters = [\n",
    "            {'params': [p for n, p in self.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01}, # original : 0.01\n",
    "            {'params': [p for n, p in self.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "        ]\n",
    "        optimizer = AdamW(optimizer_grouped_parameters, lr=5e-5) # original : 5e-5\n",
    "        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=NUM_WARMUP_STEPS, num_training_steps=NUM_TRAIN_STEPS)\n",
    "        return [optimizer],[scheduler] \n",
    "    \n",
    "    def loss_fn(self,output,target):\n",
    "        return RMSELoss()(output.view(-1,2),target.view(-1,2))\n",
    "    \n",
    "    def training_step(self,batch,batch_idx):\n",
    "        inputs = batch[\"inputs\"]\n",
    "        labels = batch[\"label\"]\n",
    "        output = self(inputs)\n",
    "        loss = self.loss_fn(output,labels)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self,batch,batch_idx):\n",
    "        inputs = batch[\"inputs\"]\n",
    "        labels = batch[\"label\"]\n",
    "        output = self(inputs)\n",
    "        loss = self.loss_fn(output,labels)\n",
    "        self.log(\"val_loss\",loss,prog_bar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize Dataset and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-05T04:08:00.347393Z",
     "iopub.status.busy": "2021-06-05T04:08:00.347027Z",
     "iopub.status.idle": "2021-06-05T04:08:00.359097Z",
     "shell.execute_reply": "2021-06-05T04:08:00.358213Z",
     "shell.execute_reply.started": "2021-06-05T04:08:00.347357Z"
    }
   },
   "outputs": [],
   "source": [
    "class BertDataset(Dataset):\n",
    "    def __init__(self,texts,labels,max_len):\n",
    "        super().__init__()\n",
    "        self.texts = texts\n",
    "        self.max_len = max_len\n",
    "        self.labels = labels\n",
    "        self.tokenizer = transformers.AutoTokenizer.from_pretrained(\"../input/roberta-transformers-pytorch/roberta-base/\")\n",
    "        #self.tokenizer = transformers.AutoTokenizer.from_pretrained(\"../input/huggingface-bert/bert-large-uncased\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.labels.shape[0]\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        text = \" \".join(self.texts[idx].split())\n",
    "        label = self.labels[idx]\n",
    "        inputs = self.tokenizer(text,return_tensors=\"pt\",max_length = self.max_len, padding=\"max_length\",truncation=True)\n",
    "        return {\n",
    "            \"inputs\":{\"input_ids\":inputs[\"input_ids\"][0],\n",
    "                      #\"token_type_ids\":inputs[\"token_type_ids\"][0],\n",
    "                      \"attention_mask\":inputs[\"attention_mask\"][0],},\n",
    "            \"label\":torch.tensor(label,dtype=torch.float)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-05T04:08:00.361849Z",
     "iopub.status.busy": "2021-06-05T04:08:00.360056Z",
     "iopub.status.idle": "2021-06-05T06:55:45.027716Z",
     "shell.execute_reply": "2021-06-05T06:55:45.025524Z",
     "shell.execute_reply.started": "2021-06-05T04:08:00.361813Z"
    }
   },
   "outputs": [],
   "source": [
    "for fold in FOLDS:\n",
    "    print(\"Fold :\",fold)\n",
    "    train_df, valid_df = df[df.kfold!=fold], df[df.kfold==fold]\n",
    "    train_dataset = BertDataset(train_df.excerpt.values,(np.array([train_df.target.values,train_df.standard_error.values]).T),max_len=max_words)\n",
    "    valid_dataset = BertDataset(valid_df.excerpt.values,(np.array([valid_df.target.values,valid_df.standard_error.values]).T),max_len=max_words)\n",
    "    train_dloader = DataLoader(train_dataset,batch_size=BATCH_SIZE,shuffle=True,num_workers=4)\n",
    "    valid_dloader = DataLoader(valid_dataset,batch_size=BATCH_SIZE,shuffle=False,num_workers=4)\n",
    "    bert_model = BertModel() \n",
    "    trainer = pl.Trainer(gpus=-1,max_epochs=EPOCHS,callbacks=[EarlyStopping(monitor=\"val_loss\",mode=\"min\",patience=15)],checkpoint_callback=False)\n",
    "    trainer.fit(model = bert_model,train_dataloader = train_dloader,val_dataloaders = valid_dloader)\n",
    "    trainer.save_checkpoint(f\"checkpoint_{fold}fold.ckpt\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# check with training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-05T06:55:45.030531Z",
     "iopub.status.busy": "2021-06-05T06:55:45.029895Z",
     "iopub.status.idle": "2021-06-05T06:58:21.201928Z",
     "shell.execute_reply": "2021-06-05T06:58:21.200889Z",
     "shell.execute_reply.started": "2021-06-05T06:55:45.030485Z"
    }
   },
   "outputs": [],
   "source": [
    "target_prediction = np.zeros(df.shape[0]) \n",
    "error_prediction = np.zeros(df.shape[0]) \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "for fold in FOLDS:\n",
    "    print(\"Fold:\",fold)\n",
    "    loaded_model = BertModel.load_from_checkpoint(f\"./checkpoint_{fold}fold.ckpt\",map_location=device)\n",
    "    loaded_model.to(device)\n",
    "    loaded_model.eval() \n",
    "    #using the same BertDataset module of train, here dummy labels are provided\n",
    "    check_dataset = BertDataset(df.excerpt.values,labels = (np.array([df.target.values,df.standard_error.values]).T),max_len=max_words)\n",
    "    check_dataloader = DataLoader(check_dataset,batch_size=BATCH_SIZE,shuffle=False,num_workers=4)\n",
    "    out_target = []\n",
    "    out_error = []\n",
    "    for batch in check_dataloader:\n",
    "        x  = batch[\"inputs\"]\n",
    "        labels = batch[\"label\"]\n",
    "        for key in x.keys():\n",
    "            x[key] = x[key].to(device)\n",
    "        assert x[\"input_ids\"].is_cuda, f\"data is not in model device({loaded_model.device.type})\"\n",
    "        out = loaded_model(x)\n",
    "        out = torch.squeeze(out, dim=1)\n",
    "        #print(out.size())\n",
    "        out_target_t = out[:,0]\n",
    "        out_error_t = out[:,1]\n",
    "        out_target.extend(out_target_t.cpu().detach().numpy())\n",
    "        out_error.extend(out_error_t.cpu().detach().numpy())\n",
    "        label_target = labels[:,0]\n",
    "        label_error = labels[:,0]\n",
    "        #print(out,labels)\n",
    "    target_prediction += np.hstack(out_target)\n",
    "    error_prediction += np.hstack(out_error)\n",
    "    #target_label += np.hstack(label_target)\n",
    "    #error_label += np.hstack(label_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-05T06:58:21.203936Z",
     "iopub.status.busy": "2021-06-05T06:58:21.203575Z",
     "iopub.status.idle": "2021-06-05T06:58:21.33508Z",
     "shell.execute_reply": "2021-06-05T06:58:21.334127Z",
     "shell.execute_reply.started": "2021-06-05T06:58:21.203896Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.scatter(target_prediction/5, df['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-05T06:58:21.336767Z",
     "iopub.status.busy": "2021-06-05T06:58:21.336389Z",
     "iopub.status.idle": "2021-06-05T06:58:21.473563Z",
     "shell.execute_reply": "2021-06-05T06:58:21.472639Z",
     "shell.execute_reply.started": "2021-06-05T06:58:21.33673Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.scatter(target_prediction/5, error_prediction/5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Weights and Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-05T06:58:21.47524Z",
     "iopub.status.busy": "2021-06-05T06:58:21.47486Z",
     "iopub.status.idle": "2021-06-05T06:58:21.645252Z",
     "shell.execute_reply": "2021-06-05T06:58:21.644421Z",
     "shell.execute_reply.started": "2021-06-05T06:58:21.475202Z"
    }
   },
   "outputs": [],
   "source": [
    "test_dataset = BertDataset(test_df.excerpt.values,labels = np.ones([test_df.shape[0],2]),max_len=max_words)\n",
    "test_dataloader = DataLoader(test_dataset,batch_size=BATCH_SIZE,shuffle=False,num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-05T06:58:21.648043Z",
     "iopub.status.busy": "2021-06-05T06:58:21.647298Z",
     "iopub.status.idle": "2021-06-05T07:00:08.427255Z",
     "shell.execute_reply": "2021-06-05T07:00:08.426185Z",
     "shell.execute_reply.started": "2021-06-05T06:58:21.647994Z"
    }
   },
   "outputs": [],
   "source": [
    "target_prediction = np.zeros(test_df.shape[0]) \n",
    "error_prediction = np.zeros(test_df.shape[0]) \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "for fold in FOLDS:\n",
    "    print(\"Fold:\",fold)\n",
    "    loaded_model = BertModel.load_from_checkpoint(f\"./checkpoint_{fold}fold.ckpt\",map_location=device)\n",
    "    loaded_model.to(device)\n",
    "    loaded_model.eval() \n",
    "    #using the same BertDataset module of train, here dummy labels are provided\n",
    "    test_dataset = BertDataset(test_df.excerpt.values,labels = np.ones([test_df.shape[0],2]),max_len=max_words)\n",
    "    test_dataloader = DataLoader(test_dataset,batch_size=BATCH_SIZE,shuffle=False,num_workers=4)\n",
    "    out_target = []\n",
    "    out_error = []\n",
    "    for batch in test_dataloader:\n",
    "        x  = batch[\"inputs\"]\n",
    "        for key in x.keys():\n",
    "            x[key] = x[key].to(device)\n",
    "        assert x[\"input_ids\"].is_cuda, f\"data is not in model device({loaded_model.device.type})\"\n",
    "        out = loaded_model(x)\n",
    "        out = torch.squeeze(out, dim=1)\n",
    "        out_target_t = out[:,0]\n",
    "        out_error_t = out[:,1]\n",
    "        out_target.extend(out_target_t.cpu().detach().numpy())\n",
    "        out_error.extend(out_error_t.cpu().detach().numpy())\n",
    "    target_prediction += np.hstack(out_target)\n",
    "    error_prediction += np.hstack(out_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-05T07:00:08.429784Z",
     "iopub.status.busy": "2021-06-05T07:00:08.429391Z",
     "iopub.status.idle": "2021-06-05T07:00:08.498979Z",
     "shell.execute_reply": "2021-06-05T07:00:08.498238Z",
     "shell.execute_reply.started": "2021-06-05T07:00:08.429743Z"
    }
   },
   "outputs": [],
   "source": [
    "test_df[\"target\"] = target_prediction/NUM_FOLDS\n",
    "sub = test_df.drop(\"excerpt\",axis=1) \n",
    "sub.to_csv(\"submission.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-05T07:00:08.50057Z",
     "iopub.status.busy": "2021-06-05T07:00:08.50021Z",
     "iopub.status.idle": "2021-06-05T07:00:08.5172Z",
     "shell.execute_reply": "2021-06-05T07:00:08.516427Z",
     "shell.execute_reply.started": "2021-06-05T07:00:08.500532Z"
    }
   },
   "outputs": [],
   "source": [
    "sub"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
