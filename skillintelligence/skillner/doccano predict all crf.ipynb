{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-13T08:20:02.028140Z",
     "start_time": "2021-07-13T08:20:02.022124Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7489,
     "status": "ok",
     "timestamp": 1617762989532,
     "user": {
      "displayName": "Shiqi Liang",
      "photoUrl": "",
      "userId": "07861028925154326261"
     },
     "user_tz": -480
    },
    "id": "LAJFvUCyb1i7",
    "outputId": "b3b32c36-835a-4b30-d6ac-360f34f39b94"
   },
   "outputs": [],
   "source": [
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-23T10:19:15.983265Z",
     "start_time": "2021-07-23T10:19:15.978235Z"
    }
   },
   "outputs": [],
   "source": [
    "#from langdetect import detect_langs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-23T10:19:16.584928Z",
     "start_time": "2021-07-23T10:19:16.573273Z"
    },
    "id": "C2DvgqUh_RfM"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seqeval'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-621fee61d559>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mseqeval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'seqeval'"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from seqeval.metrics import classification_report\n",
    "\n",
    "import torch\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from tqdm.notebook import tqdm, trange\n",
    "\n",
    "from transformers import (\n",
    "    AdamW,\n",
    "    AutoConfig,\n",
    "    AutoTokenizer,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n",
    "\n",
    "try:\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "except ImportError:\n",
    "    from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-23T10:19:17.021762Z",
     "start_time": "2021-07-23T10:19:17.015042Z"
    },
    "id": "AGmAD2tqeSy7"
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "from transformers import BertConfig\n",
    "from transformers import RobertaConfig\n",
    "from transformers import AlbertConfig, ElectraConfig\n",
    "from transformers import XLMConfig, DistilBertConfig, CamembertConfig, XLMRobertaConfig\n",
    "from transformers import AutoConfig, PretrainedConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-23T10:19:17.451058Z",
     "start_time": "2021-07-23T10:19:17.444909Z"
    },
    "id": "xLPz5HV4eS2F"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from transformers import BertModel, RobertaModel # DistilBertModel, XLMModel, AlbertModel\n",
    "from transformers import BertPreTrainedModel # AlbertPreTrainedModel, DistilBertPreTrainedModel, XLMPreTrainedModel, ElectraPreTrainedModel\n",
    "\n",
    "from typing import List, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-23T10:19:17.880792Z",
     "start_time": "2021-07-23T10:19:17.874281Z"
    },
    "id": "Lgq04Hq_eS9E"
   },
   "outputs": [],
   "source": [
    "def set_seed(args):\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    if args.n_gpu > 0:\n",
    "        torch.cuda.manual_seed_all(args.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KeHiC3NefMVC"
   },
   "source": [
    "## Input related functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-23T10:19:18.986205Z",
     "start_time": "2021-07-23T10:19:18.973763Z"
    },
    "id": "A7sp2NNueS_i"
   },
   "outputs": [],
   "source": [
    "class InputExample(object):\n",
    "    \"\"\"A single training/test example for token classification.\"\"\"\n",
    "\n",
    "    def __init__(self, guid, words, labels):\n",
    "        \"\"\"Constructs a InputExample.\n",
    "        Args:\n",
    "            guid: Unique id for the example.\n",
    "            words: list. The words of the sequence.\n",
    "            labels: (Optional) list. The labels for each word of the sequence. This should be\n",
    "            specified for train and dev examples, but not for test examples.\n",
    "        \"\"\"\n",
    "        self.guid = guid\n",
    "        self.words = words\n",
    "        self.labels = labels\n",
    "\n",
    "\n",
    "class InputFeatures(object):\n",
    "    \"\"\"A single set of features of data.\"\"\"\n",
    "\n",
    "    def __init__(self, input_ids, input_mask, valid_mask, segment_ids, label_ids, start_ids, end_ids):\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.valid_mask = valid_mask\n",
    "        self.segment_ids = segment_ids\n",
    "        self.label_ids = label_ids\n",
    "        self.start_ids = start_ids\n",
    "        self.end_ids = end_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-23T10:19:19.409876Z",
     "start_time": "2021-07-23T10:19:19.404796Z"
    },
    "id": "F79pVdQ-eTCM"
   },
   "outputs": [],
   "source": [
    "def get_labels(path):\n",
    "    if path:\n",
    "        with open(path, \"r\") as f:\n",
    "            labels = f.read().splitlines()\n",
    "        if \"O\" not in labels:\n",
    "            labels = [\"O\"] + labels\n",
    "        return labels\n",
    "    else:\n",
    "        return ['O','B-FUNC','I-FUNC','B-POWER','I-POWER','B-SAP','I-SAP','B-TECH','I-TECH','B-QUALIFICATION','I-QUALIFICATION']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-23T10:19:19.866276Z",
     "start_time": "2021-07-23T10:19:19.853556Z"
    },
    "id": "X5u09l4ofIK0"
   },
   "outputs": [],
   "source": [
    "def get_entities(seq):\n",
    "    \"\"\"Gets entities from sequence.\n",
    "    note: BIO\n",
    "    Args:\n",
    "        seq (list): sequence of labels.\n",
    "    Returns:\n",
    "        list: list of (chunk_type, chunk_start, chunk_end).\n",
    "    Example:\n",
    "        seq = ['B-PER', 'I-PER', 'O', 'B-LOC', 'I-PER']\n",
    "        get_entity_bio(seq)\n",
    "        #output\n",
    "        [['PER', 0,1], ['LOC', 3, 3], ['PER', 4, 4]]\n",
    "    \"\"\"\n",
    "    if any(isinstance(s, list) for s in seq):\n",
    "        seq = [item for sublist in seq for item in sublist + ['O']]\n",
    "\n",
    "    prev_tag = 'O'\n",
    "    prev_type = ''\n",
    "    begin_offset = 0\n",
    "    chunks = []\n",
    "    for i, chunk in enumerate(seq + ['O']):\n",
    "        tag = chunk[0]\n",
    "        type_ = chunk.split('-')[-1]\n",
    "\n",
    "        if end_of_chunk(prev_tag, tag, prev_type, type_):\n",
    "            chunks.append((prev_type, begin_offset, i - 1))\n",
    "        if start_of_chunk(prev_tag, tag, prev_type, type_):\n",
    "            begin_offset = i\n",
    "        prev_tag = tag\n",
    "        prev_type = type_\n",
    "\n",
    "    return set(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-23T10:19:20.200458Z",
     "start_time": "2021-07-23T10:19:20.184396Z"
    }
   },
   "outputs": [],
   "source": [
    "def end_of_chunk(prev_tag, tag, prev_type, type_):\n",
    "    \"\"\"Checks if a chunk ended between the previous and current word.\n",
    "    Args:\n",
    "        prev_tag: previous chunk tag.\n",
    "        tag: current chunk tag.\n",
    "        prev_type: previous type.\n",
    "        type_: current type.\n",
    "    Returns:\n",
    "        chunk_end: boolean.\n",
    "    \"\"\"\n",
    "    chunk_end = False\n",
    "\n",
    "    if prev_tag == 'E': chunk_end = True\n",
    "    if prev_tag == 'S': chunk_end = True\n",
    "\n",
    "    if prev_tag == 'B' and tag == 'B': chunk_end = True\n",
    "    if prev_tag == 'B' and tag == 'S': chunk_end = True\n",
    "    if prev_tag == 'B' and tag == 'O': chunk_end = True\n",
    "    if prev_tag == 'I' and tag == 'B': chunk_end = True\n",
    "    if prev_tag == 'I' and tag == 'S': chunk_end = True\n",
    "    if prev_tag == 'I' and tag == 'O': chunk_end = True\n",
    "\n",
    "    if prev_tag != 'O' and prev_tag != '.' and prev_type != type_:\n",
    "        chunk_end = True\n",
    "\n",
    "    return chunk_end\n",
    "\n",
    "\n",
    "def start_of_chunk(prev_tag, tag, prev_type, type_):\n",
    "    \"\"\"Checks if a chunk started between the previous and current word.\n",
    "    Args:\n",
    "        prev_tag: previous chunk tag.\n",
    "        tag: current chunk tag.\n",
    "        prev_type: previous type.\n",
    "        type_: current type.\n",
    "    Returns:\n",
    "        chunk_start: boolean.\n",
    "    \"\"\"\n",
    "    chunk_start = False\n",
    "\n",
    "    if tag == 'B': chunk_start = True\n",
    "    if tag == 'S': chunk_start = True\n",
    "\n",
    "    if prev_tag == 'E' and tag == 'E': chunk_start = True\n",
    "    if prev_tag == 'E' and tag == 'I': chunk_start = True\n",
    "    if prev_tag == 'S' and tag == 'E': chunk_start = True\n",
    "    if prev_tag == 'S' and tag == 'I': chunk_start = True\n",
    "    if prev_tag == 'O' and tag == 'E': chunk_start = True\n",
    "    if prev_tag == 'O' and tag == 'I': chunk_start = True\n",
    "\n",
    "    if tag != 'O' and tag != '.' and prev_type != type_:\n",
    "        chunk_start = True\n",
    "\n",
    "    return chunk_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-23T10:19:20.633971Z",
     "start_time": "2021-07-23T10:19:20.621238Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_entities_bio(seq):\n",
    "    \"\"\"Gets entities from sequence.\n",
    "    note: BIO\n",
    "    Args:\n",
    "        seq (list): sequence of labels.\n",
    "    Returns:\n",
    "        list: list of (chunk_type, chunk_start, chunk_end).\n",
    "    Example:\n",
    "        seq = ['B-PER', 'I-PER', 'O', 'B-LOC', 'I-PER']\n",
    "        get_entity_bio(seq)\n",
    "        #output\n",
    "        [['PER', 0,1], ['LOC', 3, 3]]\n",
    "    \"\"\"\n",
    "    if any(isinstance(s, list) for s in seq):\n",
    "        seq = [item for sublist in seq for item in sublist + ['O']]\n",
    "    chunks = []\n",
    "    chunk = [-1, -1, -1]\n",
    "    for indx, tag in enumerate(seq):\n",
    "        if tag.startswith(\"B-\"):\n",
    "            if chunk[2] != -1:\n",
    "                chunks.append(chunk)\n",
    "            chunk = [-1, -1, -1]\n",
    "            chunk[1] = indx\n",
    "            chunk[0] = tag.split('-')[1]\n",
    "            chunk[2] = indx\n",
    "            if indx == len(seq) - 1:\n",
    "                chunks.append(chunk)\n",
    "        elif tag.startswith('I-') and chunk[1] != -1:\n",
    "            _type = tag.split('-')[1]\n",
    "            if _type == chunk[0]:\n",
    "                chunk[2] = indx\n",
    "\n",
    "            if indx == len(seq) - 1:\n",
    "                chunks.append(chunk)\n",
    "        else:\n",
    "            if chunk[2] != -1:\n",
    "                chunks.append(chunk)\n",
    "            chunk = [-1, -1, -1]\n",
    "    return set([tuple(chunk) for chunk in chunks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-23T10:19:22.138675Z",
     "start_time": "2021-07-23T10:19:22.091491Z"
    },
    "id": "OkFL11eKfC04"
   },
   "outputs": [],
   "source": [
    "def convert_examples_to_features(\n",
    "        examples,\n",
    "        label_list,\n",
    "        max_seq_length,\n",
    "        tokenizer,\n",
    "        cls_token_at_end=False,\n",
    "        cls_token=\"[CLS]\",\n",
    "        cls_token_segment_id=1,\n",
    "        sep_token=\"[SEP]\",\n",
    "        sep_token_extra=False,\n",
    "        pad_on_left=False,\n",
    "        pad_token=0,\n",
    "        pad_token_segment_id=0,\n",
    "        pad_token_label_id=-100,\n",
    "        sequence_a_segment_id=0,\n",
    "        mask_padding_with_zero=True,\n",
    "):\n",
    "    \"\"\" Loads a data file into a list of `InputBatch`s\n",
    "        `cls_token_at_end` define the location of the CLS token:\n",
    "            - False (Default, BERT/XLM pattern): [CLS] + A + [SEP] + B + [SEP]\n",
    "            - True (XLNet/GPT pattern): A + [SEP] + B + [SEP] + [CLS]\n",
    "        `cls_token_segment_id` define the segment id associated to the CLS token (0 for BERT, 2 for XLNet)\n",
    "    \"\"\"\n",
    "    label_map = {label: i for i, label in enumerate(label_list)}\n",
    "    span_labels = []\n",
    "    for label in label_list:\n",
    "        label = label.split('-')[-1]\n",
    "        if label not in span_labels:\n",
    "            span_labels.append(label)\n",
    "    span_map = {label: i for i, label in enumerate(span_labels)}\n",
    "    features = []\n",
    "    for (ex_index, example) in enumerate(examples):\n",
    "#         if ex_index % 10000 == 0:\n",
    "#             logger.info(\"Writing example %d of %d\", ex_index, len(examples))\n",
    "\n",
    "        tokens = []\n",
    "        valid_mask = []\n",
    "        for word in example.words:\n",
    "            word_tokens = tokenizer.tokenize(word)\n",
    "            # bert-base-multilingual-cased sometimes output \"nothing ([]) when calling tokenize with just a space.\n",
    "            for i, word_token in enumerate(word_tokens):\n",
    "                if i == 0:\n",
    "                    valid_mask.append(1)\n",
    "                else:\n",
    "                    valid_mask.append(0)\n",
    "                tokens.append(word_token)\n",
    "        label_ids = [label_map[label] for label in example.labels]\n",
    "        entities = get_entities(example.labels)\n",
    "        start_ids = [span_map['O']] * len(label_ids)\n",
    "        end_ids = [span_map['O']] * len(label_ids)\n",
    "        for entity in entities:\n",
    "            start_ids[entity[1]] = span_map[entity[0]]\n",
    "            end_ids[entity[-1]] = span_map[entity[0]]\n",
    "        # Account for [CLS] and [SEP] with \"- 2\" and with \"- 3\" for RoBERTa.\n",
    "        special_tokens_count = 3 if sep_token_extra else 2\n",
    "        if len(tokens) > max_seq_length - special_tokens_count:\n",
    "            tokens = tokens[: (max_seq_length - special_tokens_count)]\n",
    "            label_ids = label_ids[: (max_seq_length - special_tokens_count)]\n",
    "            valid_mask = valid_mask[: (max_seq_length - special_tokens_count)]\n",
    "            start_ids = start_ids[: (max_seq_length - special_tokens_count)]\n",
    "            end_ids = end_ids[: (max_seq_length - special_tokens_count)]\n",
    "\n",
    "        tokens += [sep_token]\n",
    "        label_ids += [pad_token_label_id]\n",
    "        start_ids += [pad_token_label_id]\n",
    "        end_ids += [pad_token_label_id]\n",
    "        valid_mask.append(1)\n",
    "        if sep_token_extra:\n",
    "            # roberta uses an extra separator b/w pairs of sentences\n",
    "            tokens += [sep_token]\n",
    "            label_ids += [pad_token_label_id]\n",
    "            start_ids += [pad_token_label_id]\n",
    "            end_ids += [pad_token_label_id]\n",
    "            valid_mask.append(1)\n",
    "        segment_ids = [sequence_a_segment_id] * len(tokens)\n",
    "\n",
    "        if cls_token_at_end:\n",
    "            tokens += [cls_token]\n",
    "            label_ids += [pad_token_label_id]\n",
    "            start_ids += [pad_token_label_id]\n",
    "            end_ids += [pad_token_label_id]\n",
    "            segment_ids += [cls_token_segment_id]\n",
    "            valid_mask.append(1)\n",
    "        else:\n",
    "            tokens = [cls_token] + tokens\n",
    "            label_ids = [pad_token_label_id] + label_ids\n",
    "            start_ids = [pad_token_label_id] + start_ids\n",
    "            end_ids = [pad_token_label_id] + end_ids\n",
    "            segment_ids = [cls_token_segment_id] + segment_ids\n",
    "            valid_mask.insert(0, 1)\n",
    "\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "        # tokens are attended to.\n",
    "        input_mask = [1 if mask_padding_with_zero else 0] * len(input_ids)\n",
    "\n",
    "        # Zero-pad up to the sequence length.\n",
    "        padding_length = max_seq_length - len(input_ids)\n",
    "        if pad_on_left:\n",
    "            input_ids = ([pad_token] * padding_length) + input_ids\n",
    "            input_mask = ([0 if mask_padding_with_zero else 1] * padding_length) + input_mask\n",
    "            segment_ids = ([pad_token_segment_id] * padding_length) + segment_ids\n",
    "            label_ids = ([pad_token_label_id] * padding_length) + label_ids\n",
    "            start_ids = ([pad_token_label_id] * padding_length) + start_ids\n",
    "            end_ids = ([pad_token_label_id] * padding_length) + end_ids\n",
    "            valid_mask = ([0] * padding_length) + valid_mask\n",
    "        else:\n",
    "            input_ids += [pad_token] * padding_length\n",
    "            input_mask += [0 if mask_padding_with_zero else 1] * padding_length\n",
    "            segment_ids += [pad_token_segment_id] * padding_length\n",
    "            label_ids += [pad_token_label_id] * padding_length\n",
    "            start_ids += [pad_token_label_id] * padding_length\n",
    "            end_ids += [pad_token_label_id] * padding_length\n",
    "            valid_mask += [0] * padding_length\n",
    "        while (len(label_ids) < max_seq_length):\n",
    "            label_ids.append(pad_token_label_id)\n",
    "            start_ids.append(pad_token_label_id)\n",
    "            end_ids.append(pad_token_label_id)\n",
    "\n",
    "        assert len(input_ids) == max_seq_length\n",
    "        assert len(input_mask) == max_seq_length\n",
    "        assert len(segment_ids) == max_seq_length\n",
    "        assert len(label_ids) == max_seq_length\n",
    "        assert len(start_ids) == max_seq_length\n",
    "        assert len(end_ids) == max_seq_length\n",
    "        assert len(valid_mask) == max_seq_length\n",
    "\n",
    "#         if ex_index < 5:\n",
    "#             logger.info(\"*** Example ***\")\n",
    "#             logger.info(\"guid: %s\", example.guid)\n",
    "#             logger.info(\"tokens: %s\", \" \".join([str(x) for x in tokens]))\n",
    "#             logger.info(\"valid_mask: %s\", \" \".join([str(x) for x in valid_mask]))\n",
    "#             logger.info(\"input_ids: %s\", \" \".join([str(x) for x in input_ids]))\n",
    "#             logger.info(\"input_mask: %s\", \" \".join([str(x) for x in input_mask]))\n",
    "#             logger.info(\"segment_ids: %s\", \" \".join([str(x) for x in segment_ids]))\n",
    "#             logger.info(\"label_ids: %s\", \" \".join([str(x) for x in label_ids]))\n",
    "#             logger.info(\"start_ids: %s\", \" \".join([str(x) for x in start_ids]))\n",
    "#             logger.info(\"end_ids: %s\", \" \".join([str(x) for x in end_ids]))\n",
    "\n",
    "        features.append(\n",
    "            InputFeatures(input_ids=input_ids,\n",
    "                          input_mask=input_mask,\n",
    "                          valid_mask=valid_mask,\n",
    "                          segment_ids=segment_ids,\n",
    "                          label_ids=label_ids,\n",
    "                          start_ids=start_ids,\n",
    "                          end_ids=end_ids)\n",
    "        )\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-23T10:19:22.479563Z",
     "start_time": "2021-07-23T10:19:22.465657Z"
    },
    "id": "VSd598bSeTJ8"
   },
   "outputs": [],
   "source": [
    "def read_examples_from_file(data_dir, mode):\n",
    "    file_path = os.path.join(data_dir, \"{}.txt\".format(mode))\n",
    "    guid_index = 1\n",
    "    examples = []\n",
    "    with open(file_path, encoding=\"utf-8\") as f:\n",
    "        words = []\n",
    "        labels = []\n",
    "        for line in f:\n",
    "            if line.startswith(\"-DOCSTART-\") or line == \"\" or line == \"\\n\":\n",
    "                if words:\n",
    "                    examples.append(InputExample(guid=\"{}-{}\".format(mode, guid_index), words=words, labels=labels))\n",
    "                    guid_index += 1\n",
    "                    words = []\n",
    "                    labels = []\n",
    "            else:\n",
    "                splits = line.split(\" \")\n",
    "                words.append(splits[0])\n",
    "                if len(splits) > 1:\n",
    "                    labels.append(splits[-1].replace(\"\\n\", \"\"))\n",
    "                else:\n",
    "                    # Examples could have no label for mode = \"test\"\n",
    "                    labels.append(\"O\")\n",
    "        if words:\n",
    "            examples.append(InputExample(guid=\"{}-{}\".format(mode, guid_index), words=words, labels=labels))\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-23T10:19:24.373077Z",
     "start_time": "2021-07-23T10:19:24.353512Z"
    },
    "id": "bHsTyAOVeTHH"
   },
   "outputs": [],
   "source": [
    "def load_and_cache_examples(args, tokenizer, labels, pad_token_label_id, mode):\n",
    "    if args.local_rank not in [-1, 0] and not evaluate:\n",
    "        torch.distributed.barrier()  # Make sure only the first process in distributed training process the dataset, and the others will use the cache\n",
    "\n",
    "    # Load data features from cache or dataset file\n",
    "    cached_features_file = os.path.join(\n",
    "        args.data_dir,\n",
    "        \"cached_{}_{}_{}\".format(\n",
    "            mode, list(filter(None, args.model_name_or_path.split(\"/\"))).pop(), str(args.max_seq_length)\n",
    "        ),\n",
    "    )\n",
    "    if os.path.exists(cached_features_file) and not args.overwrite_cache:\n",
    "#         logger.info(\"Loading features from cached file %s\", cached_features_file)\n",
    "        features = torch.load(cached_features_file)\n",
    "    else:\n",
    "#         logger.info(\"Creating features from dataset file at %s\", args.data_dir)\n",
    "        examples = read_examples_from_file(args.data_dir, mode)\n",
    "        features = convert_examples_to_features(\n",
    "            examples,\n",
    "            labels,\n",
    "            args.max_seq_length,\n",
    "            tokenizer,\n",
    "            cls_token_at_end=bool(args.model_type in [\"xlnet\"]),\n",
    "            # xlnet has a cls token at the end\n",
    "            cls_token=tokenizer.cls_token,\n",
    "            cls_token_segment_id=2 if args.model_type in [\"xlnet\"] else 0,\n",
    "            sep_token=tokenizer.sep_token,\n",
    "            sep_token_extra=bool(args.model_type in [\"roberta\"]),\n",
    "            # roberta uses an extra separator b/w pairs of sentences, cf. github.com/pytorch/fairseq/commit/1684e166e3da03f5b600dbb7855cb98ddfcd0805\n",
    "            pad_on_left=bool(args.model_type in [\"xlnet\"]),\n",
    "            # pad on the left for xlnet\n",
    "            pad_token=tokenizer.pad_token_id,\n",
    "            pad_token_segment_id=tokenizer.pad_token_type_id,\n",
    "            pad_token_label_id=pad_token_label_id,\n",
    "        )\n",
    "        if args.local_rank in [-1, 0]:\n",
    "#             logger.info(\"Saving features into cached file %s\", cached_features_file)\n",
    "            torch.save(features, cached_features_file)\n",
    "\n",
    "    if args.local_rank == 0 and not evaluate:\n",
    "        torch.distributed.barrier()  # Make sure only the first process in distributed training process the dataset, and the others will use the cache\n",
    "\n",
    "    # Convert to Tensors and build dataset\n",
    "    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
    "    all_input_mask = torch.tensor([f.input_mask for f in features], dtype=torch.long)\n",
    "    all_valid_mask = torch.tensor([f.valid_mask for f in features], dtype=torch.long)\n",
    "    all_segment_ids = torch.tensor([f.segment_ids for f in features], dtype=torch.long)\n",
    "    all_label_ids = torch.tensor([f.label_ids for f in features], dtype=torch.long)\n",
    "\n",
    "    dataset = TensorDataset(all_input_ids, all_input_mask, all_valid_mask, all_segment_ids, all_label_ids)\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_vUsQi9Gg8vo"
   },
   "source": [
    "## Training related functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-23T10:19:25.072966Z",
     "start_time": "2021-07-23T10:19:25.065549Z"
    },
    "id": "gm7jXtJIfIIi"
   },
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    '''Multi-class Focal loss implementation'''\n",
    "\n",
    "    def __init__(self, gamma=2, weight=None, ignore_index=-100):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.weight = weight\n",
    "        self.ignore_index = ignore_index\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        \"\"\"\n",
    "        input: [N, C]\n",
    "        target: [N, ]\n",
    "        \"\"\"\n",
    "        logpt = F.log_softmax(input, dim=1)\n",
    "        pt = torch.exp(logpt)\n",
    "        logpt = (1 - pt) ** self.gamma * logpt\n",
    "        loss = F.nll_loss(logpt, target, self.weight, ignore_index=self.ignore_index)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-23T10:19:25.490545Z",
     "start_time": "2021-07-23T10:19:25.480634Z"
    },
    "id": "YROCRva8fIFt"
   },
   "outputs": [],
   "source": [
    "class LabelSmoothingCrossEntropy(nn.Module):\n",
    "    def __init__(self, eps=0.1, reduction='mean', ignore_index=-100):\n",
    "        super(LabelSmoothingCrossEntropy, self).__init__()\n",
    "        self.eps = eps\n",
    "        self.reduction = reduction\n",
    "        self.ignore_index = ignore_index\n",
    "\n",
    "    def forward(self, output, target):\n",
    "        c = output.size()[-1]\n",
    "        log_preds = F.log_softmax(output, dim=-1)\n",
    "        if self.reduction == 'sum':\n",
    "            loss = -log_preds.sum()\n",
    "        else:\n",
    "            loss = -log_preds.sum(dim=-1)\n",
    "            if self.reduction == 'mean':\n",
    "                loss = loss.mean()\n",
    "        return loss * self.eps / c + (1 - self.eps) * F.nll_loss(log_preds, target, reduction=self.reduction,\n",
    "                                                                 ignore_index=self.ignore_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-23T10:19:25.953781Z",
     "start_time": "2021-07-23T10:19:25.941288Z"
    },
    "id": "nzcUfne-fICg"
   },
   "outputs": [],
   "source": [
    "def valid_sequence_output(sequence_output, valid_mask, attention_mask):\n",
    "    batch_size, max_len, feat_dim = sequence_output.shape\n",
    "    valid_output = torch.zeros(batch_size, max_len, feat_dim, dtype=torch.float32,\n",
    "                               device='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    valid_attention_mask = torch.zeros(batch_size, max_len, dtype=torch.long,\n",
    "                                       device='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    for i in range(batch_size):\n",
    "        jj = -1\n",
    "        for j in range(max_len):\n",
    "            if valid_mask[i][j].item() == 1:\n",
    "                jj += 1\n",
    "                valid_output[i][jj] = sequence_output[i][j]\n",
    "                valid_attention_mask[i][jj] = attention_mask[i][j]\n",
    "    return valid_output, valid_attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-23T10:19:26.489533Z",
     "start_time": "2021-07-23T10:19:26.438513Z"
    },
    "id": "257cosgFDlIz"
   },
   "outputs": [],
   "source": [
    "class CRF(nn.Module):\n",
    "    def __init__(self, num_tags: int, batch_first: bool = False) -> None:\n",
    "        if num_tags <= 0:\n",
    "            raise ValueError(f'invalid number of tags: {num_tags}')\n",
    "        super().__init__()\n",
    "        self.num_tags = num_tags\n",
    "        self.batch_first = batch_first\n",
    "        self.start_transitions = nn.Parameter(torch.empty(num_tags))\n",
    "        self.end_transitions = nn.Parameter(torch.empty(num_tags))\n",
    "        self.transitions = nn.Parameter(torch.empty(num_tags, num_tags))\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self) -> None:\n",
    "        \"\"\"Initialize the transition parameters.\n",
    "        The parameters will be initialized randomly from a uniform distribution\n",
    "        between -0.1 and 0.1.\n",
    "        \"\"\"\n",
    "        nn.init.uniform_(self.start_transitions, -0.1, 0.1)\n",
    "        nn.init.uniform_(self.end_transitions, -0.1, 0.1)\n",
    "        nn.init.uniform_(self.transitions, -0.1, 0.1)\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f'{self.__class__.__name__}(num_tags={self.num_tags})'\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            emissions: torch.Tensor,\n",
    "            tags: Optional[torch.LongTensor] = None,\n",
    "            mask: Optional[torch.ByteTensor] = None,\n",
    "            reduction: str = 'sum'):\n",
    "        \"\"\"Compute the conditional log likelihood of a sequence of tags given emission scores.\n",
    "        Args:\n",
    "            emissions (`~torch.Tensor`): Emission score tensor of size\n",
    "                ``(seq_length, batch_size, num_tags)`` if ``batch_first`` is ``False``,\n",
    "                ``(batch_size, seq_length, num_tags)`` otherwise.\n",
    "            tags (`~torch.LongTensor`): Sequence of tags tensor of size\n",
    "                ``(seq_length, batch_size)`` if ``batch_first`` is ``False``,\n",
    "                ``(batch_size, seq_length)`` otherwise.\n",
    "            mask (`~torch.ByteTensor`): Mask tensor of size ``(seq_length, batch_size)``\n",
    "                if ``batch_first`` is ``False``, ``(batch_size, seq_length)`` otherwise.\n",
    "            reduction: Specifies  the reduction to apply to the output:\n",
    "                ``none|sum|mean|token_mean``. ``none``: no reduction will be applied.\n",
    "                ``sum``: the output will be summed over batches. ``mean``: the output will be\n",
    "                averaged over batches. ``token_mean``: the output will be averaged over tokens.\n",
    "        Returns:\n",
    "            `~torch.Tensor`: The log likelihood. This will have size ``(batch_size,)`` if\n",
    "            reduction is ``none``, ``()`` otherwise.\n",
    "        \"\"\"\n",
    "        return self._forward_alg(emissions, tags, mask, reduction)\n",
    "\n",
    "    def _forward_alg(self,\n",
    "                     emissions: torch.Tensor,\n",
    "                     tags: torch.LongTensor,\n",
    "                     mask: Optional[torch.ByteTensor] = None,\n",
    "                     reduction: str = 'sum',\n",
    "                     ) -> torch.Tensor:\n",
    "        self._validate(emissions, tags=tags, mask=mask)\n",
    "        if reduction not in ('none', 'sum', 'mean', 'token_mean'):\n",
    "            raise ValueError(f'invalid reduction: {reduction}')\n",
    "        if mask is None:\n",
    "            mask = torch.ones_like(tags, dtype=torch.uint8)\n",
    "\n",
    "        if self.batch_first:\n",
    "            emissions = emissions.transpose(0, 1)\n",
    "            tags = tags.transpose(0, 1)\n",
    "            mask = mask.transpose(0, 1)\n",
    "\n",
    "        # shape: (batch_size,)\n",
    "        numerator = self._compute_score(emissions, tags, mask)\n",
    "        # shape: (batch_size,)\n",
    "        denominator = self._compute_normalizer(emissions, mask)\n",
    "        # shape: (batch_size,)\n",
    "        llh = numerator - denominator\n",
    "\n",
    "        if reduction == 'none':\n",
    "            return llh\n",
    "        if reduction == 'sum':\n",
    "            return llh.sum()\n",
    "        if reduction == 'mean':\n",
    "            return llh.mean()\n",
    "        assert reduction == 'token_mean'\n",
    "        return llh.sum() / mask.float().sum()\n",
    "\n",
    "    def decode(self, emissions: torch.Tensor,\n",
    "               mask: Optional[torch.ByteTensor] = None) -> List[List[int]]:\n",
    "        \"\"\"Find the most likely tag sequence using Viterbi algorithm.\n",
    "        Args:\n",
    "            emissions (`~torch.Tensor`): Emission score tensor of size\n",
    "                ``(seq_length, batch_size, num_tags)`` if ``batch_first`` is ``False``,\n",
    "                ``(batch_size, seq_length, num_tags)`` otherwise.\n",
    "            mask (`~torch.ByteTensor`): Mask tensor of size ``(seq_length, batch_size)``\n",
    "                if ``batch_first`` is ``False``, ``(batch_size, seq_length)`` otherwise.\n",
    "        Returns:\n",
    "            List of list containing the best tag sequence for each batch.\n",
    "        \"\"\"\n",
    "        self._validate(emissions, mask=mask)\n",
    "        if mask is None:\n",
    "            mask = emissions.new_ones(emissions.shape[:2], dtype=torch.uint8)\n",
    "\n",
    "        if self.batch_first:\n",
    "            emissions = emissions.transpose(0, 1)\n",
    "            mask = mask.transpose(0, 1)\n",
    "\n",
    "        return self._viterbi_decode(emissions, mask)\n",
    "\n",
    "    def _validate(\n",
    "            self,\n",
    "            emissions: torch.Tensor,\n",
    "            tags: Optional[torch.LongTensor] = None,\n",
    "            mask: Optional[torch.ByteTensor] = None) -> None:\n",
    "        if emissions.dim() != 3:\n",
    "            raise ValueError(f'emissions must have dimension of 3, got {emissions.dim()}')\n",
    "        if emissions.size(2) != self.num_tags:\n",
    "            raise ValueError(\n",
    "                f'expected last dimension of emissions is {self.num_tags}, '\n",
    "                f'got {emissions.size(2)}')\n",
    "\n",
    "        if tags is not None:\n",
    "            if emissions.shape[:2] != tags.shape:\n",
    "                raise ValueError(\n",
    "                    'the first two dimensions of emissions and tags must match, '\n",
    "                    f'got {tuple(emissions.shape[:2])} and {tuple(tags.shape)}')\n",
    "\n",
    "        if mask is not None:\n",
    "            if emissions.shape[:2] != mask.shape:\n",
    "                raise ValueError(\n",
    "                    'the first two dimensions of emissions and mask must match, '\n",
    "                    f'got {tuple(emissions.shape[:2])} and {tuple(mask.shape)}')\n",
    "            no_empty_seq = not self.batch_first and mask[0].bool().all()\n",
    "            no_empty_seq_bf = self.batch_first and mask[:, 0].bool().all()\n",
    "            if not no_empty_seq and not no_empty_seq_bf:\n",
    "                raise ValueError('mask of the first timestep must all be on')\n",
    "\n",
    "    def _compute_score(\n",
    "            self, emissions: torch.Tensor, tags: torch.LongTensor,\n",
    "            mask: torch.ByteTensor) -> torch.Tensor:\n",
    "        # emissions: (seq_length, batch_size, num_tags)\n",
    "        # tags: (seq_length, batch_size)\n",
    "        # mask: (seq_length, batch_size)\n",
    "        assert emissions.dim() == 3 and tags.dim() == 2\n",
    "        assert emissions.shape[:2] == tags.shape\n",
    "        assert emissions.size(2) == self.num_tags\n",
    "        assert mask.shape == tags.shape\n",
    "        assert mask[0].bool().all()\n",
    "\n",
    "        seq_length, batch_size = tags.shape\n",
    "        mask = mask.float()\n",
    "\n",
    "        # Start transition score and first emission\n",
    "        # shape: (batch_size,)\n",
    "        score = self.start_transitions[tags[0]]\n",
    "        score += emissions[0, torch.arange(batch_size), tags[0]]\n",
    "\n",
    "        for i in range(1, seq_length):\n",
    "            # Transition score to next tag, only added if next timestep is valid (mask == 1)\n",
    "            # shape: (batch_size,)\n",
    "            score += self.transitions[tags[i - 1], tags[i]] * mask[i]\n",
    "\n",
    "            # Emission score for next tag, only added if next timestep is valid (mask == 1)\n",
    "            # shape: (batch_size,)\n",
    "            score += emissions[i, torch.arange(batch_size), tags[i]] * mask[i]\n",
    "\n",
    "        # End transition score\n",
    "        # shape: (batch_size,)\n",
    "        seq_ends = mask.long().sum(dim=0) - 1\n",
    "        # shape: (batch_size,)\n",
    "        last_tags = tags[seq_ends, torch.arange(batch_size)]\n",
    "        # shape: (batch_size,)\n",
    "        score += self.end_transitions[last_tags]\n",
    "\n",
    "        return score\n",
    "\n",
    "    def _compute_normalizer(\n",
    "            self, emissions: torch.Tensor, mask: torch.ByteTensor) -> torch.Tensor:\n",
    "        # emissions: (seq_length, batch_size, num_tags)\n",
    "        # mask: (seq_length, batch_size)\n",
    "        assert emissions.dim() == 3 and mask.dim() == 2\n",
    "        assert emissions.shape[:2] == mask.shape\n",
    "        assert emissions.size(2) == self.num_tags\n",
    "        assert mask[0].bool().all()\n",
    "        mask = mask.bool()\n",
    "\n",
    "        seq_length = emissions.size(0)\n",
    "\n",
    "        # Start transition score and first emission; score has size of\n",
    "        # (batch_size, num_tags) where for each batch, the j-th column stores\n",
    "        # the score that the first timestep has tag j\n",
    "        # shape: (batch_size, num_tags)\n",
    "        score = self.start_transitions + emissions[0]\n",
    "\n",
    "        for i in range(1, seq_length):\n",
    "            # Broadcast score for every possible next tag\n",
    "            # shape: (batch_size, num_tags, 1)\n",
    "            broadcast_score = score.unsqueeze(2)\n",
    "\n",
    "            # Broadcast emission score for every possible current tag\n",
    "            # shape: (batch_size, 1, num_tags)\n",
    "            broadcast_emissions = emissions[i].unsqueeze(1)\n",
    "\n",
    "            # Compute the score tensor of size (batch_size, num_tags, num_tags) where\n",
    "            # for each sample, entry at row i and column j stores the sum of scores of all\n",
    "            # possible tag sequences so far that end with transitioning from tag i to tag j\n",
    "            # and emitting\n",
    "            # shape: (batch_size, num_tags, num_tags)\n",
    "            next_score = broadcast_score + self.transitions + broadcast_emissions\n",
    "\n",
    "            # Sum over all possible current tags, but we're in score space, so a sum\n",
    "            # becomes a log-sum-exp: for each sample, entry i stores the sum of scores of\n",
    "            # all possible tag sequences so far, that end in tag i\n",
    "            # shape: (batch_size, num_tags)\n",
    "            next_score = torch.logsumexp(next_score, dim=1)\n",
    "\n",
    "            # Set score to the next score if this timestep is valid (mask == 1)\n",
    "            # shape: (batch_size, num_tags)\n",
    "            score = torch.where(mask[i].unsqueeze(1), next_score, score)\n",
    "\n",
    "        # End transition score\n",
    "        # shape: (batch_size, num_tags)\n",
    "        score += self.end_transitions\n",
    "\n",
    "        # Sum (log-sum-exp) over all possible tags\n",
    "        # shape: (batch_size,)\n",
    "        return torch.logsumexp(score, dim=1)\n",
    "\n",
    "    def _viterbi_decode(self, emissions: torch.FloatTensor,\n",
    "                        mask: torch.ByteTensor) -> List[List[int]]:\n",
    "        # emissions: (seq_length, batch_size, num_tags)\n",
    "        # mask: (seq_length, batch_size)\n",
    "        assert emissions.dim() == 3 and mask.dim() == 2\n",
    "        assert emissions.shape[:2] == mask.shape\n",
    "        assert emissions.size(2) == self.num_tags\n",
    "        assert mask[0].bool().all()\n",
    "        mask = mask.bool()\n",
    "\n",
    "        seq_length, batch_size = mask.shape\n",
    "\n",
    "        # Start transition and first emission\n",
    "        # shape: (batch_size, num_tags)\n",
    "        score = self.start_transitions + emissions[0]\n",
    "        history = []\n",
    "\n",
    "        # score is a tensor of size (batch_size, num_tags) where for every batch,\n",
    "        # value at column j stores the score of the best tag sequence so far that ends\n",
    "        # with tag j\n",
    "        # history saves where the best tags candidate transitioned from; this is used\n",
    "        # when we trace back the best tag sequence\n",
    "\n",
    "        # Viterbi algorithm recursive case: we compute the score of the best tag sequence\n",
    "        # for every possible next tag\n",
    "        for i in range(1, seq_length):\n",
    "            # Broadcast viterbi score for every possible next tag\n",
    "            # shape: (batch_size, num_tags, 1)\n",
    "            broadcast_score = score.unsqueeze(2)\n",
    "\n",
    "            # Broadcast emission score for every possible current tag\n",
    "            # shape: (batch_size, 1, num_tags)\n",
    "            broadcast_emission = emissions[i].unsqueeze(1)\n",
    "\n",
    "            # Compute the score tensor of size (batch_size, num_tags, num_tags) where\n",
    "            # for each sample, entry at row i and column j stores the score of the best\n",
    "            # tag sequence so far that ends with transitioning from tag i to tag j and emitting\n",
    "            # shape: (batch_size, num_tags, num_tags)\n",
    "            next_score = broadcast_score + self.transitions + broadcast_emission\n",
    "\n",
    "            # Find the maximum score over all possible current tag\n",
    "            # shape: (batch_size, num_tags)\n",
    "            next_score, indices = next_score.max(dim=1)\n",
    "\n",
    "            # Set score to the next score if this timestep is valid (mask == 1)\n",
    "            # and save the index that produces the next score\n",
    "            # shape: (batch_size, num_tags)\n",
    "            score = torch.where(mask[i].unsqueeze(1), next_score, score)\n",
    "            history.append(indices)\n",
    "\n",
    "        # End transition score\n",
    "        # shape: (batch_size, num_tags)\n",
    "        score += self.end_transitions\n",
    "\n",
    "        # Now, compute the best path for each sample\n",
    "\n",
    "        # shape: (batch_size,)\n",
    "        seq_ends = mask.long().sum(dim=0) - 1\n",
    "        best_tags_list = []\n",
    "\n",
    "        for idx in range(batch_size):\n",
    "            # Find the tag which maximizes the score at the last timestep; this is our best tag\n",
    "            # for the last timestep\n",
    "            _, best_last_tag = score[idx].max(dim=0)\n",
    "            best_tags = [best_last_tag.item()]\n",
    "\n",
    "            # We trace back where the best last tag comes from, append that to our best tag\n",
    "            # sequence, and trace it back again, and so on\n",
    "            for hist in reversed(history[:seq_ends[idx]]):\n",
    "                best_last_tag = hist[idx][best_tags[-1]]\n",
    "                best_tags.append(best_last_tag.item())\n",
    "\n",
    "            # Reverse the order because we start from the last timestep\n",
    "            best_tags.reverse()\n",
    "            best_tags_list.append(best_tags)\n",
    "        best_tags_list = [item + [-1] * (seq_length - len(item)) for item in best_tags_list]\n",
    "        best_tags_list = torch.from_numpy(np.array(best_tags_list))\n",
    "        return torch.LongTensor(best_tags_list).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-23T10:19:26.834171Z",
     "start_time": "2021-07-23T10:19:26.818447Z"
    },
    "id": "bh17xgjxfH-p"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BertPreTrainedModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-9b0c7fc6c865>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mBertCrfForNer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBertPreTrainedModel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBertCrfForNer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_dropout_prob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'BertPreTrainedModel' is not defined"
     ]
    }
   ],
   "source": [
    "class BertCrfForNer(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super(BertCrfForNer, self).__init__(config)\n",
    "        self.bert = BertModel(config)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "        self.crf = CRF(num_tags=config.num_labels, batch_first=True)\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            input_ids,\n",
    "            attention_mask=None,\n",
    "            token_type_ids=None,\n",
    "            position_ids=None,\n",
    "            head_mask=None,\n",
    "            inputs_embeds=None,\n",
    "            valid_mask=None,\n",
    "            labels=None,\n",
    "            decode=False,\n",
    "    ):\n",
    "        outputs = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds\n",
    "        )\n",
    "        sequence_output = outputs[0]\n",
    "        sequence_output, attention_mask = valid_sequence_output(sequence_output, valid_mask, attention_mask)\n",
    "        sequence_output = self.dropout(sequence_output)\n",
    "        logits = self.classifier(sequence_output)\n",
    "        if decode:\n",
    "            tags = self.crf.decode(logits, attention_mask)\n",
    "            outputs = (tags,)\n",
    "        else:\n",
    "            outputs = (logits,)\n",
    "\n",
    "        if labels is not None:\n",
    "            labels = torch.where(labels >= 0, labels, torch.zeros_like(labels))\n",
    "            loss = self.crf(emissions=logits, tags=labels, mask=attention_mask)\n",
    "            outputs = (-1 * loss,) + outputs\n",
    "\n",
    "        return outputs  # (loss), scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-23T10:19:27.271927Z",
     "start_time": "2021-07-23T10:19:27.258593Z"
    },
    "id": "L2n_anqEhPVW"
   },
   "outputs": [],
   "source": [
    "class AutoModelForCrfNer:\n",
    "    def __init__(self):\n",
    "        raise EnvironmentError(\n",
    "            \"AutoModelForTokenClassification is designed to be instantiated \"\n",
    "            \"using the `AutoModelForTokenClassification.from_pretrained(pretrained_model_name_or_path)` or \"\n",
    "            \"`AutoModelForTokenClassification.from_config(config)` methods.\"\n",
    "        )\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        for config_class, model_class in MODEL_FOR_CRF_NER_MAPPING.items():\n",
    "            if isinstance(config, config_class):\n",
    "                return model_class(config)\n",
    "\n",
    "        raise ValueError(\n",
    "            \"Unrecognized configuration class {} for this kind of AutoModel: {}.\\n\"\n",
    "            \"Model type should be one of {}.\".format(\n",
    "                config.__class__,\n",
    "                cls.__name__,\n",
    "                \", \".join(c.__name__ for c in MODEL_FOR_CRF_NER_MAPPING.keys()),\n",
    "            )\n",
    "        )\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs):\n",
    "        config = kwargs.pop(\"config\", None)\n",
    "        if not isinstance(config, PretrainedConfig):\n",
    "            config = AutoConfig.from_pretrained(pretrained_model_name_or_path, **kwargs)\n",
    "\n",
    "        for config_class, model_class in MODEL_FOR_CRF_NER_MAPPING.items():\n",
    "            if isinstance(config, config_class):\n",
    "                return model_class.from_pretrained(pretrained_model_name_or_path, *model_args, config=config, **kwargs)\n",
    "\n",
    "        raise ValueError(\n",
    "            \"Unrecognized configuration class {} for this kind of AutoModel: {}.\\n\"\n",
    "            \"Model type should be one of {}.\".format(\n",
    "                config.__class__,\n",
    "                cls.__name__,\n",
    "                \", \".join(c.__name__ for c in MODEL_FOR_CRF_NER_MAPPING.keys()),\n",
    "            )\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-23T10:19:27.701985Z",
     "start_time": "2021-07-23T10:19:27.696938Z"
    },
    "id": "zbBzjLkXhPPu"
   },
   "outputs": [],
   "source": [
    "MODEL_FOR_CRF_NER_MAPPING = OrderedDict(\n",
    "    [\n",
    "        # (XLMConfig, XLMCrfForNer),\n",
    "        # (DistilBertConfig, DistilBertCrfForNer),\n",
    "        # (RobertaConfig, RobertaCrfForNer),\n",
    "        # (CamembertConfig, RobertaCrfForNer),\n",
    "        # (XLMRobertaConfig, RobertaCrfForNer),\n",
    "        (BertConfig, BertCrfForNer),\n",
    "        # (AlbertConfig, AlbertCrfForNer),\n",
    "        # (ElectraConfig, ElectraCrfForNer),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-23T10:19:28.183913Z",
     "start_time": "2021-07-23T10:19:28.175015Z"
    },
    "id": "8zW44ifBhPSW"
   },
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "MODEL_CONFIG_CLASSES = list(MODEL_FOR_CRF_NER_MAPPING.keys())\n",
    "MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)\n",
    "ALL_MODELS = sum((tuple(conf.pretrained_config_archive_map.keys()) for conf in MODEL_CONFIG_CLASSES), ())\n",
    "TOKENIZER_ARGS = [\"do_lower_case\", \"strip_accents\", \"keep_accents\", \"use_fast\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-23T10:19:29.069627Z",
     "start_time": "2021-07-23T10:19:29.066018Z"
    },
    "id": "Xl4d8F9lDAMi"
   },
   "outputs": [],
   "source": [
    "# classifier_parameters = [(n,p) for (n,p) in model.named_parameters() if args.model_type not in n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-23T10:19:29.808690Z",
     "start_time": "2021-07-23T10:19:29.759411Z"
    },
    "id": "i3oDCcivhPM5"
   },
   "outputs": [],
   "source": [
    "def train(args, train_dataset, model, tokenizer, labels, pad_token_label_id):\n",
    "    \"\"\" Train the model \"\"\"\n",
    "    if args.local_rank in [-1, 0]:\n",
    "        tb_writer = SummaryWriter(args.output_dir)\n",
    "\n",
    "    args.train_batch_size = args.per_gpu_train_batch_size * max(1, args.n_gpu)\n",
    "    train_sampler = RandomSampler(train_dataset) if args.local_rank == -1 else DistributedSampler(train_dataset)\n",
    "    train_dataloader = DataLoader(train_dataset,\n",
    "                                  sampler=train_sampler,\n",
    "                                  batch_size=args.train_batch_size,\n",
    "                                  collate_fn=collate_fn)\n",
    "\n",
    "    if args.max_steps > 0:\n",
    "        t_total = args.max_steps\n",
    "        args.num_train_epochs = args.max_steps // (len(train_dataloader) // args.gradient_accumulation_steps) + 1\n",
    "    else:\n",
    "        t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n",
    "\n",
    "    args.logging_steps = eval(args.logging_steps)\n",
    "    if isinstance(args.logging_steps, float):\n",
    "        args.logging_steps = int(args.logging_steps * len(train_dataloader)) // args.gradient_accumulation_steps\n",
    "\n",
    "    # Prepare optimizer and schedule (linear warmup and decay)\n",
    "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "    bert_parameters = eval('model.{}'.format(args.model_type)).named_parameters()\n",
    "    classifier_parameters = model.classifier.named_parameters()\n",
    "    crf_parameters = model.crf.named_parameters()\n",
    "    args.bert_lr = args.bert_lr if args.bert_lr else args.learning_rate\n",
    "    args.classifier_lr = args.classifier_lr if args.classifier_lr else args.learning_rate\n",
    "    args.crf_lr = args.crf_lr if args.crf_lr else args.learning_rate\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\"params\": [p for n, p in bert_parameters if not any(nd in n for nd in no_decay)],\n",
    "         \"weight_decay\": args.weight_decay,\n",
    "         \"lr\": args.bert_lr},\n",
    "        {\"params\": [p for n, p in bert_parameters if any(nd in n for nd in no_decay)],\n",
    "         \"weight_decay\": 0.0,\n",
    "         \"lr\": args.bert_lr},\n",
    "\n",
    "        {\"params\": [p for n, p in classifier_parameters if not any(nd in n for nd in no_decay)],\n",
    "         \"weight_decay\": args.weight_decay,\n",
    "         \"lr\": args.classifier_lr},\n",
    "        {\"params\": [p for n, p in classifier_parameters if any(nd in n for nd in no_decay)],\n",
    "         \"weight_decay\": 0.0,\n",
    "         \"lr\": args.classifier_lr},\n",
    "\n",
    "        {\"params\": [p for n, p in crf_parameters if not any(nd in n for nd in no_decay)],\n",
    "         \"weight_decay\": args.weight_decay,\n",
    "         \"lr\": args.crf_lr},\n",
    "        {\"params\": [p for n, p in crf_parameters if any(nd in n for nd in no_decay)],\n",
    "         \"weight_decay\": 0.0,\n",
    "         \"lr\": args.crf_lr},\n",
    "    ]\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total\n",
    "    )\n",
    "\n",
    "    # Check if saved optimizer or scheduler states exist\n",
    "    if os.path.isfile(os.path.join(args.model_name_or_path, \"optimizer.pt\")) and os.path.isfile(\n",
    "            os.path.join(args.model_name_or_path, \"scheduler.pt\")\n",
    "    ):\n",
    "        # Load in optimizer and scheduler states\n",
    "        optimizer.load_state_dict(torch.load(os.path.join(args.model_name_or_path, \"optimizer.pt\")))\n",
    "        scheduler.load_state_dict(torch.load(os.path.join(args.model_name_or_path, \"scheduler.pt\")))\n",
    "\n",
    "    # multi-gpu training (should be after apex fp16 initialization)\n",
    "    if args.n_gpu > 1:\n",
    "        model = torch.nn.DataParallel(model)\n",
    "\n",
    "    # Distributed training (should be after apex fp16 initialization)\n",
    "    if args.local_rank != -1:\n",
    "        model = torch.nn.parallel.DistributedDataParallel(\n",
    "            model, device_ids=[args.local_rank], output_device=args.local_rank, find_unused_parameters=True\n",
    "        )\n",
    "\n",
    "    # adversarial_training\n",
    "    if args.adv_training == 'fgm':\n",
    "        adv = FGM(model=model, param_name='word_embeddings')\n",
    "    elif args.adv_training == 'pgd':\n",
    "        adv = PGD(model=model, param_name='word_embeddings')\n",
    "\n",
    "    # Train!\n",
    "    logger.info(\"***** Running training *****\")\n",
    "    logger.info(\"  Num examples = %d\", len(train_dataset))\n",
    "    logger.info(\"  Num Epochs = %d\", args.num_train_epochs)\n",
    "    logger.info(\"  Instantaneous batch size per GPU = %d\", args.per_gpu_train_batch_size)\n",
    "    logger.info(\n",
    "        \"  Total train batch size (w. parallel, distributed & accumulation) = %d\",\n",
    "        args.train_batch_size\n",
    "        * args.gradient_accumulation_steps\n",
    "        * (torch.distributed.get_world_size() if args.local_rank != -1 else 1),\n",
    "    )\n",
    "    logger.info(\"  Gradient Accumulation steps = %d\", args.gradient_accumulation_steps)\n",
    "    logger.info(\"  Total optimization steps = %d\", t_total)\n",
    "\n",
    "    global_step = 0\n",
    "    epochs_trained = 0\n",
    "    best_score = 0.0\n",
    "    steps_trained_in_current_epoch = 0\n",
    "    # Check if continuing training from a checkpoint\n",
    "    if os.path.exists(args.model_name_or_path):\n",
    "        # set global_step to gobal_step of last saved checkpoint from model path\n",
    "        try:\n",
    "            global_step = int(args.model_name_or_path.split(\"-\")[-1].split(\"/\")[0])\n",
    "        except ValueError:\n",
    "            global_step = 0\n",
    "        epochs_trained = global_step // (len(train_dataloader) // args.gradient_accumulation_steps)\n",
    "        steps_trained_in_current_epoch = global_step % (len(train_dataloader) // args.gradient_accumulation_steps)\n",
    "\n",
    "        logger.info(\"  Continuing training from checkpoint, will skip to saved global_step\")\n",
    "        logger.info(\"  Continuing training from epoch %d\", epochs_trained)\n",
    "        logger.info(\"  Continuing training from global step %d\", global_step)\n",
    "        logger.info(\"  Will skip the first %d steps in the first epoch\", steps_trained_in_current_epoch)\n",
    "\n",
    "    tr_loss, logging_loss = 0.0, 0.0\n",
    "    model.zero_grad()\n",
    "    train_iterator = trange(\n",
    "        epochs_trained, int(args.num_train_epochs), desc=\"Epoch\", disable=args.local_rank not in [-1, 0]\n",
    "    )\n",
    "    set_seed(args)  # Added here for reproductibility\n",
    "    for _ in train_iterator:\n",
    "        epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\", disable=args.local_rank not in [-1, 0])\n",
    "        for step, batch in enumerate(epoch_iterator):\n",
    "\n",
    "            # Skip past any already trained steps if resuming training\n",
    "            if steps_trained_in_current_epoch > 0:\n",
    "                steps_trained_in_current_epoch -= 1\n",
    "                continue\n",
    "\n",
    "            model.train()\n",
    "            batch = tuple(t.to(args.device) for t in batch)\n",
    "            inputs = {\"input_ids\": batch[0],\n",
    "                      \"attention_mask\": batch[1],\n",
    "                      \"valid_mask\": batch[2],\n",
    "                      \"labels\": batch[4], }\n",
    "            if args.model_type != \"distilbert\":\n",
    "                inputs[\"token_type_ids\"] = (\n",
    "                    batch[3] if args.model_type in [\"bert\", \"xlnet\"] else None\n",
    "                )  # XLM and RoBERTa don\"t use segment_ids\n",
    "\n",
    "            outputs = model(**inputs)\n",
    "            loss = outputs[0]  # model outputs are always tuple in pytorch-transformers (see doc)\n",
    "\n",
    "            if args.n_gpu > 1:\n",
    "                loss = loss.mean()  # mean() to average on multi-gpu parallel training\n",
    "            if args.gradient_accumulation_steps > 1:\n",
    "                loss = loss / args.gradient_accumulation_steps\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            if args.adv_training:\n",
    "                adv.adversarial_training(args, inputs, optimizer)\n",
    "\n",
    "            tr_loss += loss.item()\n",
    "            epoch_iterator.set_description('Loss: {}'.format(round(loss.item(), 6)))\n",
    "            if (step + 1) % args.gradient_accumulation_steps == 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
    "\n",
    "                optimizer.step()\n",
    "                scheduler.step()  # Update learning rate schedule\n",
    "                model.zero_grad()\n",
    "                global_step += 1\n",
    "\n",
    "                if args.local_rank in [-1, 0] and args.logging_steps > 0 and global_step % args.logging_steps == 0:\n",
    "                    # Log metrics\n",
    "                    if (\n",
    "                            args.local_rank == -1 and args.evaluate_during_training\n",
    "                    ):  # Only evaluate when single GPU otherwise metrics may not average well\n",
    "                        results, _ = evaluate(args, model, tokenizer, labels, pad_token_label_id, mode=\"dev\",\n",
    "                                              prefix=global_step)\n",
    "                        for key, value in results.items():\n",
    "                            if isinstance(value, float) or isinstance(value, int):\n",
    "                                tb_writer.add_scalar(\"eval_{}\".format(key), value, global_step)\n",
    "                    tb_writer.add_scalar(\"lr\", scheduler.get_lr()[0], global_step)\n",
    "                    tb_writer.add_scalar(\"loss\", (tr_loss - logging_loss) / args.logging_steps, global_step)\n",
    "                    logging_loss = tr_loss\n",
    "\n",
    "                    if best_score < results['f1']:\n",
    "                        best_score = results['f1']\n",
    "                        output_dir = os.path.join(args.output_dir, \"best_checkpoint\")\n",
    "                        if not os.path.exists(output_dir):\n",
    "                            os.makedirs(output_dir)\n",
    "                        model_to_save = (\n",
    "                            model.module if hasattr(model, \"module\") else model\n",
    "                        )  # Take care of distributed/parallel training\n",
    "                        model_to_save.save_pretrained(output_dir)\n",
    "                        tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "                        torch.save(args, os.path.join(output_dir, \"training_args.bin\"))\n",
    "                        logger.info(\"Saving model checkpoint to %s\", output_dir)\n",
    "\n",
    "                        torch.save(optimizer.state_dict(), os.path.join(output_dir, \"optimizer.pt\"))\n",
    "                        torch.save(scheduler.state_dict(), os.path.join(output_dir, \"scheduler.pt\"))\n",
    "                        logger.info(\"Saving optimizer and scheduler states to %s\", output_dir)\n",
    "\n",
    "            if args.max_steps > 0 and global_step > args.max_steps:\n",
    "                epoch_iterator.close()\n",
    "                break\n",
    "        if args.max_steps > 0 and global_step > args.max_steps:\n",
    "            train_iterator.close()\n",
    "            break\n",
    "\n",
    "    if args.local_rank in [-1, 0]:\n",
    "        tb_writer.close()\n",
    "\n",
    "    return global_step, tr_loss / global_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-23T10:19:30.311077Z",
     "start_time": "2021-07-23T10:19:30.286863Z"
    }
   },
   "outputs": [],
   "source": [
    "def f1_score(true_entities, pred_entities):\n",
    "    \"\"\"Compute the F1 score.\"\"\"\n",
    "    nb_correct = len(true_entities & pred_entities)\n",
    "    nb_pred = len(pred_entities)\n",
    "    nb_true = len(true_entities)\n",
    "\n",
    "    p = nb_correct / nb_pred if nb_pred > 0 else 0\n",
    "    r = nb_correct / nb_true if nb_true > 0 else 0\n",
    "    score = 2 * p * r / (p + r) if p + r > 0 else 0\n",
    "\n",
    "    return score\n",
    "\n",
    "\n",
    "def precision_score(true_entities, pred_entities):\n",
    "    \"\"\"Compute the precision.\"\"\"\n",
    "    nb_correct = len(true_entities & pred_entities)\n",
    "    nb_pred = len(pred_entities)\n",
    "\n",
    "    score = nb_correct / nb_pred if nb_pred > 0 else 0\n",
    "\n",
    "    return score\n",
    "\n",
    "\n",
    "def recall_score(true_entities, pred_entities):\n",
    "    \"\"\"Compute the recall.\"\"\"\n",
    "    nb_correct = len(true_entities & pred_entities)\n",
    "    nb_true = len(true_entities)\n",
    "\n",
    "    score = nb_correct / nb_true if nb_true > 0 else 0\n",
    "\n",
    "    return score\n",
    "\n",
    "\n",
    "def classification_report(true_entities, pred_entities, digits=5):\n",
    "    \"\"\"Build a text report showing the main classification metrics.\"\"\"\n",
    "    name_width = 0\n",
    "    d1 = defaultdict(set)\n",
    "    d2 = defaultdict(set)\n",
    "    for e in true_entities:\n",
    "        d1[e[0]].add((e[1], e[2]))\n",
    "        name_width = max(name_width, len(e[0]))\n",
    "    for e in pred_entities:\n",
    "        d2[e[0]].add((e[1], e[2]))\n",
    "\n",
    "    last_line_heading = 'macro avg'\n",
    "    width = max(name_width, len(last_line_heading), digits)\n",
    "\n",
    "    headers = [\"precision\", \"recall\", \"f1-score\", \"support\"]\n",
    "    head_fmt = u'{:>{width}s} ' + u' {:>9}' * len(headers)\n",
    "    report = head_fmt.format(u'', *headers, width=width)\n",
    "    report += u'\\n\\n'\n",
    "\n",
    "    row_fmt = u'{:>{width}s} ' + u' {:>9.{digits}f}' * 3 + u' {:>9}\\n'\n",
    "\n",
    "    ps, rs, f1s, s = [], [], [], []\n",
    "    for type_name, type_true_entities in d1.items():\n",
    "        type_pred_entities = d2[type_name]\n",
    "        nb_correct = len(type_true_entities & type_pred_entities)\n",
    "        nb_pred = len(type_pred_entities)\n",
    "        nb_true = len(type_true_entities)\n",
    "\n",
    "        p = nb_correct / nb_pred if nb_pred > 0 else 0\n",
    "        r = nb_correct / nb_true if nb_true > 0 else 0\n",
    "        f1 = 2 * p * r / (p + r) if p + r > 0 else 0\n",
    "\n",
    "        report += row_fmt.format(*[type_name, p, r, f1, nb_true], width=width, digits=digits)\n",
    "\n",
    "        ps.append(p)\n",
    "        rs.append(r)\n",
    "        f1s.append(f1)\n",
    "        s.append(nb_true)\n",
    "\n",
    "    report += u'\\n'\n",
    "\n",
    "    # compute averages\n",
    "    report += row_fmt.format('micro avg',\n",
    "                             precision_score(true_entities, pred_entities),\n",
    "                             recall_score(true_entities, pred_entities),\n",
    "                             f1_score(true_entities, pred_entities),\n",
    "                             np.sum(s),\n",
    "                             width=width, digits=digits)\n",
    "    report += row_fmt.format(last_line_heading,\n",
    "                             np.average(ps, weights=s),\n",
    "                             np.average(rs, weights=s),\n",
    "                             np.average(f1s, weights=s),\n",
    "                             np.sum(s),\n",
    "                             width=width, digits=digits)\n",
    "\n",
    "    return report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-23T10:19:31.270044Z",
     "start_time": "2021-07-23T10:19:31.265779Z"
    },
    "id": "ryPRvbqBhPKN"
   },
   "outputs": [],
   "source": [
    "# def custom_metric(true_entities, pred_entities):\n",
    "#     nb_correct = len(true_entities & pred_entities)\n",
    "#     nb_pred = len(pred_entities)\n",
    "#     nb_true = len(true_entities)\n",
    "\n",
    "#     p = nb_correct / nb_pred if nb_pred > 0 else 0\n",
    "#     r = nb_correct / nb_true if nb_true > 0 else 0\n",
    "#     score = 2 * p * r / (p + r) if p + r > 0 else 0\n",
    "\n",
    "#     return p, r, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-23T10:19:31.834375Z",
     "start_time": "2021-07-23T10:19:31.806391Z"
    },
    "id": "fTYb2ksohhY8"
   },
   "outputs": [],
   "source": [
    "def evaluate(args, model, tokenizer, labels, pad_token_label_id, mode, prefix=\"\"):\n",
    "    eval_dataset = load_and_cache_examples(args, tokenizer, labels, pad_token_label_id, mode=mode)\n",
    "\n",
    "    args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n",
    "    # Note that DistributedSampler samples randomly\n",
    "    eval_sampler = SequentialSampler(eval_dataset) if args.local_rank == -1 else DistributedSampler(eval_dataset)\n",
    "    eval_dataloader = DataLoader(eval_dataset,\n",
    "                                 sampler=eval_sampler,\n",
    "                                 batch_size=args.eval_batch_size)\n",
    "\n",
    "    # multi-gpu evaluate\n",
    "    if args.n_gpu > 1 and not isinstance(model, torch.nn.DataParallel):\n",
    "        model = torch.nn.DataParallel(model)\n",
    "\n",
    "    # Eval!\n",
    "    logger.info(\"***** Running evaluation %s *****\", prefix)\n",
    "    logger.info(\"  Num examples = %d\", len(eval_dataset))\n",
    "    logger.info(\"  Batch size = %d\", args.eval_batch_size)\n",
    "    eval_loss = 0.0\n",
    "    nb_eval_steps = 0\n",
    "    preds = None\n",
    "    trues = None\n",
    "    model.eval()\n",
    "    for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
    "        batch = tuple(t.to(args.device) for t in batch)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            inputs = {\"input_ids\": batch[0],\n",
    "                      \"attention_mask\": batch[1],\n",
    "                      \"valid_mask\": batch[2],\n",
    "                      \"labels\": batch[4],\n",
    "                      \"decode\": True}\n",
    "            if args.model_type != \"distilbert\":\n",
    "                inputs[\"token_type_ids\"] = (\n",
    "                    batch[2] if args.model_type in [\"bert\", \"xlnet\"] else None\n",
    "                )  # XLM and RoBERTa don\"t use segment_ids\n",
    "            outputs = model(**inputs)\n",
    "            tmp_eval_loss, tags = outputs[:2]\n",
    "            if args.n_gpu > 1:\n",
    "                tmp_eval_loss = tmp_eval_loss.mean()  # mean() to average on multi-gpu parallel evaluating\n",
    "            eval_loss += tmp_eval_loss.item()\n",
    "        nb_eval_steps += 1\n",
    "        if preds is None:\n",
    "            preds = tags.detach().cpu().numpy()\n",
    "            trues = inputs[\"labels\"].detach().cpu().numpy()\n",
    "        else:\n",
    "            preds = np.append(preds, tags.detach().cpu().numpy(), axis=0)\n",
    "            trues = np.append(trues, inputs[\"labels\"].detach().cpu().numpy(), axis=0)\n",
    "\n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    label_map = {i: label for i, label in enumerate(labels)}\n",
    "\n",
    "    trues_list = [[] for _ in range(trues.shape[0])]\n",
    "    preds_list = [[] for _ in range(preds.shape[0])]\n",
    "\n",
    "    for i in range(trues.shape[0]):\n",
    "        for j in range(trues.shape[1]):\n",
    "            if trues[i, j] != pad_token_label_id:\n",
    "                trues_list[i].append(label_map[trues[i][j]])\n",
    "                preds_list[i].append(label_map[preds[i][j]])\n",
    "\n",
    "    true_entities = get_entities_bio(trues_list)\n",
    "    pred_entities = get_entities_bio(preds_list)\n",
    "    results = {\n",
    "        \"loss\": eval_loss,\n",
    "        \"f1\": f1_score(true_entities, pred_entities),\n",
    "        'report': classification_report(true_entities, pred_entities)\n",
    "    }\n",
    "\n",
    "    output_eval_file = os.path.join(args.output_dir, \"eval_results.txt\")\n",
    "    if not os.path.exists(args.output_dir):\n",
    "        os.makedirs(args.output_dir)\n",
    "    with open(output_eval_file, \"a\") as writer:\n",
    "        logger.info(\"***** Eval results {} *****\".format(prefix))\n",
    "        writer.write(\"***** Eval results {} *****\\n\".format(prefix))\n",
    "        writer.write(\"***** Eval loss : {} *****\\n\".format(eval_loss))\n",
    "        for key in sorted(results.keys()):\n",
    "            if key == 'report_dict':\n",
    "                continue\n",
    "            logger.info(\"{} = {}\".format(key, str(results[key])))\n",
    "            writer.write(\"{} = {}\\n\".format(key, str(results[key])))\n",
    "    return results, preds_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-23T10:19:32.396299Z",
     "start_time": "2021-07-23T10:19:32.368619Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict(args, model, tokenizer, labels, pad_token_label_id, mode, prefix=\"\"):\n",
    "    eval_dataset = load_and_cache_examples(args, tokenizer, labels, pad_token_label_id, mode=mode)\n",
    "\n",
    "    args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n",
    "    # Note that DistributedSampler samples randomly\n",
    "    eval_sampler = SequentialSampler(eval_dataset) if args.local_rank == -1 else DistributedSampler(eval_dataset)\n",
    "    eval_dataloader = DataLoader(eval_dataset,\n",
    "                                 sampler=eval_sampler,\n",
    "                                 batch_size=args.eval_batch_size)\n",
    "\n",
    "    # multi-gpu evaluate\n",
    "    if args.n_gpu > 1 and not isinstance(model, torch.nn.DataParallel):\n",
    "        model = torch.nn.DataParallel(model)\n",
    "\n",
    "    # Eval!\n",
    "    logger.info(\"***** Running evaluation %s *****\", prefix)\n",
    "    logger.info(\"  Num examples = %d\", len(eval_dataset))\n",
    "    logger.info(\"  Batch size = %d\", args.eval_batch_size)\n",
    "    eval_loss = 0.0\n",
    "    nb_eval_steps = 0\n",
    "    preds = None\n",
    "    trues = None\n",
    "    model.eval()\n",
    "    for batch in tqdm(eval_dataloader, desc=\"Evaluating\", position=0):\n",
    "#     for batch in eval_dataloader:\n",
    "        batch = tuple(t.to(args.device) for t in batch)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            inputs = {\"input_ids\": batch[0],\n",
    "                      \"attention_mask\": batch[1],\n",
    "                      \"valid_mask\": batch[2],\n",
    "                      \"labels\": batch[4], }\n",
    "            if args.model_type != \"distilbert\":\n",
    "                inputs[\"token_type_ids\"] = (\n",
    "                    batch[2] if args.model_type in [\"bert\", \"xlnet\"] else None\n",
    "                )  # XLM and RoBERTa don\"t use segment_ids\n",
    "            outputs = model(**inputs)\n",
    "            tmp_eval_loss, logits = outputs[:2]\n",
    "            logits = F.softmax(logits, dim=-1)\n",
    "            if args.n_gpu > 1:\n",
    "                tmp_eval_loss = tmp_eval_loss.mean()  # mean() to average on multi-gpu parallel evaluating\n",
    "            eval_loss += tmp_eval_loss.item()\n",
    "        nb_eval_steps += 1\n",
    "        if preds is None:\n",
    "            preds = logits.detach().cpu().numpy()\n",
    "            trues = inputs[\"labels\"].detach().cpu().numpy()\n",
    "        else:\n",
    "            preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n",
    "            trues = np.append(trues, inputs[\"labels\"].detach().cpu().numpy(), axis=0)\n",
    "\n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    preds_max = np.max(preds, axis=2)\n",
    "    preds = np.argmax(preds, axis=2)\n",
    "    label_map = {i: label for i, label in enumerate(labels)}\n",
    "\n",
    "    trues_list = [[] for _ in range(trues.shape[0])]\n",
    "    preds_list = [[] for _ in range(preds.shape[0])]\n",
    "\n",
    "    for i in range(trues.shape[0]):\n",
    "        for j in range(trues.shape[1]):\n",
    "            if trues[i, j] != pad_token_label_id:\n",
    "                trues_list[i].append(label_map[trues[i][j]])\n",
    "                preds_list[i].append(label_map[preds[i][j]])\n",
    "                \n",
    "    lab_pred_chunks_list = []\n",
    "    lab_pred_chunks_prob = []\n",
    "    for i in range(trues.shape[0]):\n",
    "        # We use the get chunks function defined above to get the true chunks\n",
    "        # and the predicted chunks from true labels and predicted labels respectively\n",
    "        lab_pred_chunks = get_entities_bio([preds_list[i]])\n",
    "        lab_prob = []\n",
    "        lab_pred_chunks_new = []\n",
    "        if len(lab_pred_chunks)>0:\n",
    "            for l in lab_pred_chunks:\n",
    "                lab_prob.append(np.min(preds_max[i][l[1]:l[2]+1]))\n",
    "                lab_pred_chunks_new.append(l)\n",
    "        lab_pred_chunks_list.append(lab_pred_chunks_new)\n",
    "        lab_pred_chunks_prob.append(lab_prob)\n",
    "    return lab_pred_chunks_list, lab_pred_chunks_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-23T10:19:36.145397Z",
     "start_time": "2021-07-23T10:19:36.141191Z"
    },
    "id": "W4qWqkQydENR"
   },
   "outputs": [],
   "source": [
    "# def test_metric(label, pred, beta=2):\n",
    "#     match_label = set()\n",
    "#     manual_only = set()\n",
    "#     pred_only = set()\n",
    "\n",
    "#     match_label = set(label).intersection(pred)\n",
    "\n",
    "#     pred_only = set(pred) - match_label\n",
    "#     manual_only = set(label) - match_label\n",
    "\n",
    "#     precision = len(match_label)/len(set(pred)) if len(set(pred)) else 0\n",
    "#     recall = len(match_label)/len(set(label))\n",
    "#     f1 = (1+beta**2)*precision*recall/((beta**2)*precision+recall) if ((beta**2)*precision+recall) else 0\n",
    "\n",
    "#     return [list(match_label), list(manual_only), list(pred_only), precision, recall, f1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-23T10:19:36.448668Z",
     "start_time": "2021-07-23T10:19:36.438743Z"
    },
    "id": "gOLbvN0rcagx"
   },
   "outputs": [],
   "source": [
    "# def test_evaluation(args, model, tokenizer, labels, pad_token_label_id, mode=\"test\", prefix='test'):\n",
    "#     eval_dataset = load_and_cache_examples(args, tokenizer, labels, pad_token_label_id, mode=mode)\n",
    "\n",
    "#     args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n",
    "#     # Note that DistributedSampler samples randomly\n",
    "#     eval_sampler = SequentialSampler(eval_dataset)\n",
    "#     eval_dataloader = DataLoader(eval_dataset,\n",
    "#                                  sampler=eval_sampler,\n",
    "#                                  batch_size=args.eval_batch_size)\n",
    "\n",
    "#     # multi-gpu evaluate\n",
    "#     if args.n_gpu > 1 and not isinstance(model, torch.nn.DataParallel):\n",
    "#         model = torch.nn.DataParallel(model)\n",
    "\n",
    "#     # Eval!\n",
    "#     logger.info(\"***** Running evaluation %s *****\", prefix)\n",
    "#     logger.info(\"  Num examples = %d\", len(eval_dataset))\n",
    "#     logger.info(\"  Batch size = %d\", args.eval_batch_size)\n",
    "#     eval_loss = 0.0\n",
    "#     nb_eval_steps = 0\n",
    "#     preds = None\n",
    "#     trues = None\n",
    "#     model.eval()\n",
    "#     for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
    "#         batch = tuple(t.to(args.device) for t in batch)\n",
    "\n",
    "#         with torch.no_grad():\n",
    "#             inputs = {\"input_ids\": batch[0],\n",
    "#                       \"attention_mask\": batch[1],\n",
    "#                       \"valid_mask\": batch[2],\n",
    "#                       \"labels\": batch[4], }\n",
    "#             if args.model_type != \"distilbert\":\n",
    "#                 inputs[\"token_type_ids\"] = (\n",
    "#                     batch[2] if args.model_type in [\"bert\", \"xlnet\"] else None\n",
    "#                 )  # XLM and RoBERTa don\"t use segment_ids\n",
    "#             outputs = model(**inputs)\n",
    "#             tmp_eval_loss, logits = outputs[:2]\n",
    "#             if args.n_gpu > 1:\n",
    "#                 tmp_eval_loss = tmp_eval_loss.mean()  # mean() to average on multi-gpu parallel evaluating\n",
    "#             eval_loss += tmp_eval_loss.item()\n",
    "#         nb_eval_steps += 1\n",
    "#         if preds is None:\n",
    "#             preds = logits.detach().cpu().numpy()\n",
    "#             trues = inputs[\"labels\"].detach().cpu().numpy()\n",
    "#         else:\n",
    "#             preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n",
    "#             trues = np.append(trues, inputs[\"labels\"].detach().cpu().numpy(), axis=0)\n",
    "\n",
    "#     eval_loss = eval_loss / nb_eval_steps\n",
    "#     preds = np.argmax(preds, axis=2)\n",
    "#     label_map = {i: label for i, label in enumerate(labels)}\n",
    "\n",
    "#     trues_list = [[] for _ in range(trues.shape[0])]\n",
    "#     preds_list = [[] for _ in range(preds.shape[0])]\n",
    "\n",
    "#     for i in range(trues.shape[0]):\n",
    "#         for j in range(trues.shape[1]):\n",
    "#             if trues[i, j] != pad_token_label_id:\n",
    "#                 trues_list[i].append(label_map[trues[i][j]])\n",
    "#                 preds_list[i].append(label_map[preds[i][j]])\n",
    "                \n",
    "#     # Save predictions\n",
    "#     output_test_predictions_file = os.path.join(args.output_dir, \"test_predictions.txt\")\n",
    "#     with open(output_test_predictions_file, \"w\") as writer:\n",
    "#         with open(os.path.join(args.data_dir, \"test.txt\"), \"r\") as f:\n",
    "#             example_id = 0\n",
    "#             for line in f:\n",
    "#                 if line.startswith(\"-DOCSTART-\") or line == \"\" or line == \"\\n\":\n",
    "#                     writer.write(line)\n",
    "#                     if not preds_list[example_id]:\n",
    "#                         example_id += 1\n",
    "#                 elif preds_list[example_id]:\n",
    "#                     output_line = line.split()[0] + \" \" + preds_list[example_id].pop(0) + \"\\n\"\n",
    "#                     writer.write(output_line)\n",
    "#                 else:\n",
    "#                     logger.warning(\"Maximum sequence length exceeded: No prediction for '%s'.\", line.split()[0])\n",
    "    \n",
    "#     test = pd.read_csv('/content/drive/MyDrive/skill scraper/datasets/test_df.csv', index_col=0)\n",
    "#     test['original_index'] = test['original_index'].str.lower()\n",
    "\n",
    "#     test['end_idx'] = test['start_idx']+test['token'].apply(lambda x: len(x))\n",
    "\n",
    "#     pred=[]\n",
    "#     with open('/content/drive/MyDrive/skill scraper/model_crf/test_predictions.txt', 'r') as f:\n",
    "#         for line in f:\n",
    "#             if line=='\\n':\n",
    "#                 pass\n",
    "#             else:\n",
    "#                 pred.append([line.split()[0], line.split()[1]])\n",
    "\n",
    "#     pred = pd.DataFrame(pred, columns=['pred_token', 'pred_tag'])\n",
    "\n",
    "#     test['space']=False\n",
    "#     for i in test.index:\n",
    "#         if len(tokenizer.tokenize(test.loc[i, 'token']))==0:\n",
    "#             test.loc[i, 'space']=True\n",
    "#     test = test[test['space']==False]\n",
    "\n",
    "#     test['pred_token'] = pred['pred_token'].values\n",
    "#     test['pred_tag'] = pred['pred_tag'].values\n",
    "#     pred['original_index'] = test['original_index'].values\n",
    "#     pred['original_index'] = pred['original_index'].str.lower()\n",
    "\n",
    "#     pred_tags = {}\n",
    "#     for name_i, group_i in test.groupby('original_index'):\n",
    "#         pred_tag = []\n",
    "#         if group_i['pred_tag'].iloc[0] in ['SAP','TECH','SOFT']:\n",
    "#             pred_tag.append(group_i['pred_token'].iloc[0])\n",
    "#         for idx in range(1, len(group_i)):\n",
    "#             if group_i['pred_tag'].iloc[idx] in ['SAP','TECH','SOFT']:\n",
    "#                 if group_i['pred_tag'].iloc[idx] != group_i['pred_tag'].iloc[idx-1]:\n",
    "#                     pred_tag.append(group_i['pred_token'].iloc[idx])\n",
    "#                 elif group_i['pred_tag'].iloc[idx] == group_i['pred_tag'].iloc[idx-1]:\n",
    "#                     pred_tag[-1] = pred_tag[-1]+(' ')*(group_i['start_idx'].iloc[idx]-group_i['end_idx'].iloc[idx-1])+group_i['pred_token'].iloc[idx]\n",
    "#         pred_tags[name_i] = set(pred_tag)\n",
    "\n",
    "#     labels = pd.read_csv('/content/drive/MyDrive/skill scraper/total_ground_truth.csv', index_col=0)\n",
    "#     # labels = labels[(labels['company']=='sap')&(labels['work_area'].isin(['Consulting','Customer Support']))]\n",
    "#     # print('labels shape: ', labels.shape)\n",
    "\n",
    "#     model_pred_list = []\n",
    "#     match_label_list = []\n",
    "#     manual_only_list = []\n",
    "#     pred_only_list = []\n",
    "#     precision_list = []\n",
    "#     recall_list = []\n",
    "#     f1_list = []\n",
    "\n",
    "#     for idx in labels.index:\n",
    "#         label = labels.loc[idx, 'Skills']\n",
    "#         label = label.split(',')\n",
    "#         label = [i.lower().strip() for i in label]\n",
    "#         label = [i for i in label if len(i)>0]\n",
    "#         pred = list(pred_tags[labels.loc[idx, 'original_index']])\n",
    "#         pred = [i.lower().strip() for i in pred]\n",
    "#         match_label,manual_only,pred_only,precision,recall,f1 = test_metric(label, pred, beta=2)\n",
    "#         model_pred_list.append(pred)\n",
    "#         match_label_list.append(match_label)\n",
    "#         manual_only_list.append(manual_only)\n",
    "#         pred_only_list.append(pred_only)\n",
    "#         precision_list.append(precision)\n",
    "#         recall_list.append(recall)\n",
    "#         f1_list.append(f1)\n",
    "\n",
    "#     labels['pred_label'] = model_pred_list\n",
    "#     labels['match_label'] = match_label_list\n",
    "#     labels['manual_only'] = manual_only_list\n",
    "#     labels['pred_only'] = pred_only_list\n",
    "#     labels['precision'] = precision_list\n",
    "#     labels['recall'] = recall_list\n",
    "#     labels['f1'] = f1_list\n",
    "\n",
    "#     # print(labels[['pred_label','match_label']])\n",
    "\n",
    "#     return labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-23T10:19:37.224312Z",
     "start_time": "2021-07-23T10:19:37.214539Z"
    },
    "id": "0ZGDdIophhVT"
   },
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    batch should be a list of (sequence, target, length) tuples...\n",
    "    Returns a padded tensor of sequences sorted from longest to shortest,\n",
    "    \"\"\"\n",
    "    batch_tuple = tuple(map(torch.stack, zip(*batch)))\n",
    "    batch_lens = torch.sum(batch_tuple[1], dim=-1, keepdim=False)\n",
    "    max_len = batch_lens.max().item()\n",
    "    results = ()\n",
    "    for item in batch_tuple:\n",
    "        if item.dim() >= 2:\n",
    "            results += (item[:, :max_len],)\n",
    "        else:\n",
    "            results += (item,)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-23T10:19:37.889943Z",
     "start_time": "2021-07-23T10:19:37.859025Z"
    },
    "id": "5IwGJ3OuhhR2"
   },
   "outputs": [],
   "source": [
    "def loss_backward(args, loss, optimizer):\n",
    "    if args.n_gpu > 1:\n",
    "        loss = loss.mean()  # mean() to average on multi-gpu parallel training\n",
    "    loss.backward()\n",
    "\n",
    "class FGM():\n",
    "    def __init__(self, model, param_name, alpha=1.0):\n",
    "        self.model = model\n",
    "        self.param_name = param_name\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def adversarial(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad and self.param_name in name:\n",
    "                norm = torch.norm(param.grad)\n",
    "                if norm != 0 and not torch.isnan(norm):\n",
    "                    perturbation = self.alpha * param.grad / norm\n",
    "                    param.data.add_(perturbation)\n",
    "\n",
    "    def backup_param_data(self):\n",
    "        self.data = {}\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad and self.param_name in name:\n",
    "                self.data[name] = param.data.clone()\n",
    "\n",
    "    def restore_param_data(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad and self.param_name in name:\n",
    "                assert name in self.data\n",
    "                param.data = self.data[name]\n",
    "        self.data = {}\n",
    "\n",
    "    def adversarial_training(self, args, inputs, optimizer):\n",
    "        self.backup_param_data()\n",
    "        self.adversarial()\n",
    "        loss = self.model(**inputs)[0]\n",
    "        loss_backward(args, loss, optimizer)\n",
    "        self.restore_param_data()\n",
    "\n",
    "\n",
    "class PGD():\n",
    "    def __init__(self, model, param_name, alpha=0.3, epsilon=1.0, K=3):\n",
    "        self.model = model\n",
    "        self.param_name = param_name\n",
    "        self.alpha = alpha\n",
    "        self.epsilon = epsilon\n",
    "        self.K = K\n",
    "\n",
    "    def backup_param_data(self):\n",
    "        self.data = {}\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad and self.param_name in name:\n",
    "                self.data[name] = param.data.clone()\n",
    "\n",
    "    def restore_param_data(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad and self.param_name in name:\n",
    "                param.data = self.data[name]\n",
    "\n",
    "    def backup_param_grad(self):\n",
    "        self.grad = {}\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad and self.param_name in name:\n",
    "                self.grad[name] = param.grad.clone()\n",
    "\n",
    "    def restore_param_grad(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad and self.param_name in name:\n",
    "                param.grad = self.grad[name]\n",
    "\n",
    "\n",
    "    def adversarial(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad and self.param_name in name:\n",
    "                norm = torch.norm(param.grad)\n",
    "                if norm != 0 and not torch.isnan(norm):\n",
    "                    perturbation = self.alpha * param.grad / norm\n",
    "                    param.data.add_(perturbation)\n",
    "                    param.data = self.project(name, param.data)\n",
    "\n",
    "    def project(self, param_name, param_data):\n",
    "        eta = torch.clamp(param_data - self.data[param_name])\n",
    "        norm = torch.norm(eta)\n",
    "        if norm > self.epsilon:\n",
    "            eta = self.epsilon * eta / norm\n",
    "        return self.data[param_name] + eta\n",
    "\n",
    "    def adversarial_training(self, args, inputs, optimizer):\n",
    "        self.backup_param_data()\n",
    "        self.backup_param_grad()\n",
    "        for k in range(self.K):\n",
    "            self.adversarial()\n",
    "            if k != self.K - 1:\n",
    "                self.model.zero_grad()\n",
    "            else:\n",
    "                self.restore_param_grad()\n",
    "            loss = self.model(**inputs)[0]\n",
    "            loss_backward(args, loss, optimizer)\n",
    "        self.restore_param_data()\n",
    "\n",
    "\n",
    "class FreeAT():\n",
    "    def __init__(self, model, param_name, alpha=0.3, epsilon=1.0, K=3):\n",
    "        self.model = model\n",
    "        self.param_name = param_name\n",
    "        self.alpha = alpha\n",
    "        self.epsilon = epsilon\n",
    "        self.K = K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-23T10:19:39.619679Z",
     "start_time": "2021-07-23T10:19:39.605241Z"
    },
    "id": "ECglkKOGhg2R"
   },
   "outputs": [],
   "source": [
    "class args:\n",
    "    data_dir = None\n",
    "    model_type = None\n",
    "    model_name_or_path = None\n",
    "    output_dir = None\n",
    "    labels = None\n",
    "    config_name = \"\"\n",
    "    tokenizer_name = \"\"\n",
    "    cache_dir = \"\"\n",
    "    max_seq_length = 128\n",
    "    do_train = True\n",
    "    do_eval = True\n",
    "    do_predict = True\n",
    "    evaluate_during_training = True\n",
    "    do_lower_case = True\n",
    "    per_gpu_train_batch_size = 8\n",
    "    per_gpu_eval_batch_size = 8\n",
    "    gradient_accumulation_steps = 1\n",
    "    loss_type = 'ce'\n",
    "    learning_rate = 5e-5\n",
    "    bert_lr = 5e-5\n",
    "    classifier_lr = 5e-5\n",
    "    crf_lr = 1e-3\n",
    "    adv_training = None\n",
    "    weight_decay = 0.0\n",
    "    adam_epsilon = 1e-8\n",
    "    max_grad_norm = 1.0\n",
    "    num_train_epochs = 3.0\n",
    "    max_steps = -1\n",
    "    warmup_steps = 0\n",
    "    logging_steps = '0.1'\n",
    "    overwrite_output_dir = True\n",
    "    no_cuda = False\n",
    "    seed = 42\n",
    "    overwrite_cache = False\n",
    "    local_rank = -1\n",
    "\n",
    "args.data_dir = './pred_data'\n",
    "args.model_type = 'bert'\n",
    "args.model_name_or_path = './model_crf_uncased_tech/best_checkpoint'\n",
    "args.output_dir = './model_crf_v1/best_checkpoint'\n",
    "args.labels = ''\n",
    "args.overwrite_output_dir = True\n",
    "args.overwrite_cache = True\n",
    "args.do_train = False\n",
    "args.do_eval = False\n",
    "args.do_predict = True\n",
    "args.evaluate_during_training = False\n",
    "args.num_train_epochs = 100\n",
    "args.max_seq_length = 512\n",
    "args.logging_steps = '0.5'\n",
    "args.per_gpu_train_batch_size = 8\n",
    "args.per_gpu_eval_batch_size = 16\n",
    "args.learning_rate = 5e-5\n",
    "args.bert_lr = 5e-5\n",
    "args.classifier_lr = 5e-5\n",
    "args.crf_lr = 1e-3\n",
    "args.do_lower_case = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-23T10:20:11.070214Z",
     "start_time": "2021-07-23T10:19:41.412595Z"
    },
    "id": "kKaipSjsi8LW"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/23/2021 10:19:41 - WARNING - __main__ -   device: cuda, n_gpu: 1\n",
      "07/23/2021 10:19:41 - INFO - transformers.configuration_utils -   loading configuration file ./model_crf_uncased_tech/best_checkpoint/config.json\n",
      "07/23/2021 10:19:41 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
      "  \"_num_labels\": 11,\n",
      "  \"architectures\": [\n",
      "    \"BertCrfForNer\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bad_words_ids\": null,\n",
      "  \"bos_token_id\": null,\n",
      "  \"decoder_start_token_id\": null,\n",
      "  \"do_sample\": false,\n",
      "  \"early_stopping\": false,\n",
      "  \"eos_token_id\": null,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"O\",\n",
      "    \"1\": \"B-FUNC\",\n",
      "    \"10\": \"I-QUALIFICATION\",\n",
      "    \"2\": \"I-FUNC\",\n",
      "    \"3\": \"B-POWER\",\n",
      "    \"4\": \"I-POWER\",\n",
      "    \"5\": \"B-SAP\",\n",
      "    \"6\": \"I-SAP\",\n",
      "    \"7\": \"B-TECH\",\n",
      "    \"8\": \"I-TECH\",\n",
      "    \"9\": \"B-QUALIFICATION\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"B-FUNC\": 1,\n",
      "    \"B-POWER\": 3,\n",
      "    \"B-QUALIFICATION\": 9,\n",
      "    \"B-SAP\": 5,\n",
      "    \"B-TECH\": 7,\n",
      "    \"I-FUNC\": 2,\n",
      "    \"I-POWER\": 4,\n",
      "    \"I-QUALIFICATION\": 10,\n",
      "    \"I-SAP\": 6,\n",
      "    \"I-TECH\": 8,\n",
      "    \"O\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"loss_type\": \"ce\",\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"min_length\": 0,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"prefix\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"task_specific_params\": null,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "07/23/2021 10:19:41 - INFO - __main__ -   Tokenizer arguments: {'do_lower_case': True}\n",
      "07/23/2021 10:19:41 - INFO - transformers.configuration_utils -   loading configuration file ./model_crf_uncased_tech/best_checkpoint/config.json\n",
      "07/23/2021 10:19:41 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
      "  \"_num_labels\": 11,\n",
      "  \"architectures\": [\n",
      "    \"BertCrfForNer\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bad_words_ids\": null,\n",
      "  \"bos_token_id\": null,\n",
      "  \"decoder_start_token_id\": null,\n",
      "  \"do_sample\": false,\n",
      "  \"early_stopping\": false,\n",
      "  \"eos_token_id\": null,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"O\",\n",
      "    \"1\": \"B-FUNC\",\n",
      "    \"2\": \"I-FUNC\",\n",
      "    \"3\": \"B-POWER\",\n",
      "    \"4\": \"I-POWER\",\n",
      "    \"5\": \"B-SAP\",\n",
      "    \"6\": \"I-SAP\",\n",
      "    \"7\": \"B-TECH\",\n",
      "    \"8\": \"I-TECH\",\n",
      "    \"9\": \"B-QUALIFICATION\",\n",
      "    \"10\": \"I-QUALIFICATION\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"B-FUNC\": 1,\n",
      "    \"B-POWER\": 3,\n",
      "    \"B-QUALIFICATION\": 9,\n",
      "    \"B-SAP\": 5,\n",
      "    \"B-TECH\": 7,\n",
      "    \"I-FUNC\": 2,\n",
      "    \"I-POWER\": 4,\n",
      "    \"I-QUALIFICATION\": 10,\n",
      "    \"I-SAP\": 6,\n",
      "    \"I-TECH\": 8,\n",
      "    \"O\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"loss_type\": \"ce\",\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"min_length\": 0,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"prefix\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"task_specific_params\": null,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "07/23/2021 10:19:41 - INFO - transformers.tokenization_utils -   Model name './model_crf_uncased_tech/best_checkpoint' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming './model_crf_uncased_tech/best_checkpoint' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "07/23/2021 10:19:41 - INFO - transformers.tokenization_utils -   Didn't find file ./model_crf_uncased_tech/best_checkpoint/added_tokens.json. We won't load it.\n",
      "07/23/2021 10:19:41 - INFO - transformers.tokenization_utils -   loading file ./model_crf_uncased_tech/best_checkpoint/vocab.txt\n",
      "07/23/2021 10:19:41 - INFO - transformers.tokenization_utils -   loading file None\n",
      "07/23/2021 10:19:41 - INFO - transformers.tokenization_utils -   loading file ./model_crf_uncased_tech/best_checkpoint/special_tokens_map.json\n",
      "07/23/2021 10:19:41 - INFO - transformers.tokenization_utils -   loading file ./model_crf_uncased_tech/best_checkpoint/tokenizer_config.json\n",
      "07/23/2021 10:19:41 - INFO - transformers.modeling_utils -   loading weights file ./model_crf_uncased_tech/best_checkpoint/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels:  ['O', 'B-FUNC', 'I-FUNC', 'B-POWER', 'I-POWER', 'B-SAP', 'I-SAP', 'B-TECH', 'I-TECH', 'B-QUALIFICATION', 'I-QUALIFICATION']\n",
      "pad_token_label_id:  -100\n",
      "Tokenizer arguments: %s {'do_lower_case': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/23/2021 10:20:11 - INFO - __main__ -   Training/evaluation parameters <class '__main__.args'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training/evaluation parameters %s <class '__main__.args'>\n"
     ]
    }
   ],
   "source": [
    "if (\n",
    "        os.path.exists(args.output_dir)\n",
    "        and os.listdir(args.output_dir)\n",
    "        and args.do_train\n",
    "        and not args.overwrite_output_dir\n",
    "):\n",
    "    raise ValueError(\n",
    "        \"Output directory ({}) already exists and is not empty. Use --overwrite_output_dir to overcome.\".format(\n",
    "            args.output_dir\n",
    "        )\n",
    "    )\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n",
    "args.n_gpu = 0 if args.no_cuda else torch.cuda.device_count()\n",
    "args.device = device\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    level=logging.INFO \n",
    ")\n",
    "logger.warning(\n",
    "    \"device: %s, n_gpu: %s\",\n",
    "    device,\n",
    "    args.n_gpu,\n",
    ")\n",
    "\n",
    "# Set seed\n",
    "set_seed(args)\n",
    "\n",
    "# Prepare CONLL-2003 task\n",
    "labels = get_labels(args.labels)\n",
    "num_labels = len(labels)\n",
    "# Use cross entropy ignore index as padding label id so that only real label ids contribute to the loss later\n",
    "pad_token_label_id = CrossEntropyLoss().ignore_index\n",
    "print('labels: ', labels)\n",
    "print('pad_token_label_id: ', pad_token_label_id)\n",
    "\n",
    "args.model_type = args.model_type.lower()\n",
    "config = AutoConfig.from_pretrained(\n",
    "    args.config_name if args.config_name else args.model_name_or_path,\n",
    "    num_labels=num_labels,\n",
    "    id2label={str(i): label for i, label in enumerate(labels)},\n",
    "    label2id={label: i for i, label in enumerate(labels)},\n",
    "    cache_dir=args.cache_dir if args.cache_dir else None,\n",
    ")\n",
    "#####\n",
    "setattr(config, 'loss_type', args.loss_type)\n",
    "# setattr(config, 'output_hidden_states', True)\n",
    "#####\n",
    "tokenizer_args = {k: v for k, v in vars(args).items() if v is not None and k in TOKENIZER_ARGS}\n",
    "print(\"Tokenizer arguments: %s\", tokenizer_args)\n",
    "logger.info(\"Tokenizer arguments: %s\", tokenizer_args)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    args.tokenizer_name if args.tokenizer_name else args.model_name_or_path,\n",
    "    cache_dir=args.cache_dir if args.cache_dir else None,\n",
    "    **tokenizer_args,\n",
    ")\n",
    "model = AutoModelForCrfNer.from_pretrained(\n",
    "    args.model_name_or_path,\n",
    "    from_tf=bool(\".ckpt\" in args.model_name_or_path),\n",
    "    config=config,\n",
    "    cache_dir=args.cache_dir if args.cache_dir else None,\n",
    ")\n",
    "\n",
    "model.to(args.device)\n",
    "\n",
    "logger.info(\"Training/evaluation parameters %s\", args)\n",
    "print(\"Training/evaluation parameters %s\", args)\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(args.output_dir, **tokenizer_args)\n",
    "# checkpoint = os.path.join(args.output_dir, 'best_checkpoint')\n",
    "# model = AutoModelForCrfNer.from_pretrained(checkpoint)\n",
    "# model.to(args.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-23T10:20:15.511890Z",
     "start_time": "2021-07-23T10:20:11.112942Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/23/2021 10:20:11 - INFO - pytorch_transformers.modeling_bert -   Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n",
      "07/23/2021 10:20:11 - INFO - pytorch_transformers.modeling_xlnet -   Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-30T11:26:27.285902Z",
     "start_time": "2021-06-30T11:26:26.843466Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-23T10:20:15.549371Z",
     "start_time": "2021-07-23T10:20:15.546648Z"
    }
   },
   "outputs": [],
   "source": [
    "# company='google'\n",
    "# df=pd.read_csv(f'./cleaned_data/{company}_jobs.csv', index_col=0)\n",
    "# df=df[df['work_area'].isin(['SOFTWARE_ENGINEERING','TECHNICAL_SOLUTIONS'])]\n",
    "# df=df.sample(n=10)\n",
    "# df=df.dropna(subset=['description_raw'])\n",
    "# for i in df.index:\n",
    "#     d = detect_langs(df.loc[i,'description_raw'])\n",
    "#     df.loc[i,'lang_cnt'] = len(d)\n",
    "#     df.loc[i,'primary_lang'] = d[0].lang\n",
    "    \n",
    "# df = df[(df['primary_lang']=='en')&(df['lang_cnt']==1)]\n",
    "# df = df.reset_index(drop=True)\n",
    "# df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-23T10:20:15.655338Z",
     "start_time": "2021-07-23T10:20:15.578675Z"
    }
   },
   "outputs": [],
   "source": [
    "# df=pd.read_csv('validation_jobs_small.csv', index_col=0)\n",
    "# df['labels']=df['labels'].map(eval)\n",
    "# df=df.reset_index(drop=True)\n",
    "# for i in df.index:\n",
    "#     d = detect_langs(df.loc[i,'text'])\n",
    "#     df.loc[i,'lang_cnt'] = len(d)\n",
    "#     df.loc[i,'primary_lang'] = d[0].lang\n",
    "    \n",
    "# df = df[(df['primary_lang']=='en')&(df['lang_cnt']==1)]\n",
    "# df = df.reset_index(drop=True)\n",
    "# df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-23T10:20:15.759105Z",
     "start_time": "2021-07-23T10:20:15.686092Z"
    }
   },
   "outputs": [],
   "source": [
    "# df['work_area'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-23T10:20:15.892369Z",
     "start_time": "2021-07-23T10:20:15.789098Z"
    }
   },
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "# df=[]\n",
    "# with open('./evaluate/project_1_dataset.jsonl', 'r') as f:\n",
    "#     for line in f:\n",
    "#         df.append(json.loads(line))\n",
    "        \n",
    "# df = pd.DataFrame(df)\n",
    "# df = df[df['comment_count']==1]\n",
    "# df['company'] = df['meta'].apply(lambda x: x['company'])\n",
    "# df['work_area'] = df['meta'].apply(lambda x: x['work_area'])\n",
    "# df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-23T10:20:18.617069Z",
     "start_time": "2021-07-23T10:20:15.923588Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1740, 3)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./cleaned_data/sap_jobs.csv',index_col=0)\n",
    "df = df.dropna(subset=['work_area'])\n",
    "df['work_area']=df['work_area'].str.lower().str.strip()\n",
    "# df['flag']=df['work_area'].str.contains('|'.join(['technology','development']))\n",
    "df['work_area']=df['work_area'].apply(lambda x: re.sub(r'location.+','',x))\n",
    "df['work_area']=df['work_area'].apply(lambda x: re.sub(r'expected travel:.+','',x))\n",
    "df['work_area']=df['work_area'].str.lower().str.strip()\n",
    "df = df[df['work_area'].isin(['solution and product management','software-research','software-quality assurance','software-user experience'])]\n",
    "df = df.reset_index(drop=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-23T10:20:18.657089Z",
     "start_time": "2021-07-23T10:20:18.645690Z"
    }
   },
   "outputs": [],
   "source": [
    "df['text'] = df['description_raw'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-12T12:22:07.736895Z",
     "start_time": "2021-07-12T12:22:07.700715Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-23T10:31:22.605800Z",
     "start_time": "2021-07-23T10:20:18.686682Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 0th prediction batch:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/23/2021 10:22:17 - INFO - __main__ -   ***** Running evaluation test *****\n",
      "07/23/2021 10:22:17 - INFO - __main__ -     Num examples = 1171\n",
      "07/23/2021 10:22:17 - INFO - __main__ -     Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd28a7590c1444bab48dcfecf5fd3f42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/74 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 1th prediction batch:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/23/2021 10:25:23 - INFO - __main__ -   ***** Running evaluation test *****\n",
      "07/23/2021 10:25:23 - INFO - __main__ -     Num examples = 1216\n",
      "07/23/2021 10:25:23 - INFO - __main__ -     Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b27bf2c900f24f80bb20572dd5de6834",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/76 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 2th prediction batch:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/23/2021 10:28:38 - INFO - __main__ -   ***** Running evaluation test *****\n",
      "07/23/2021 10:28:38 - INFO - __main__ -     Num examples = 1258\n",
      "07/23/2021 10:28:38 - INFO - __main__ -     Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9317c2a25aa442ebb5e55c09df7a1e00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 3th prediction batch:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/23/2021 10:30:49 - INFO - __main__ -   ***** Running evaluation test *****\n",
      "07/23/2021 10:30:49 - INFO - __main__ -     Num examples = 604\n",
      "07/23/2021 10:30:49 - INFO - __main__ -     Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d96723b2f24347468338c0baefc3a71e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/38 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i_batch in range(0,(len(df)//500)+1):\n",
    "    \n",
    "    print(f'The {i_batch}th prediction batch:')\n",
    "    temp = df.iloc[(i_batch*500):((i_batch+1)*500)]\n",
    "    data = []\n",
    "    for i in temp.index:\n",
    "        text = temp.loc[i, 'description_raw']\n",
    "#         text = temp.loc[i, 'text']\n",
    "        doc = nlp(text)\n",
    "        for token in doc:\n",
    "            data.append([i, token.text, token.idx, token.pos_, token.is_sent_start])\n",
    "    data = pd.DataFrame(data, columns=['original_index', 'token', 'start_idx', 'pos', 'sent_start'])\n",
    "    data['token_len'] = data['token'].apply(lambda x: len(x))\n",
    "    data['end_idx'] = data['token_len']+data['start_idx']\n",
    "    data['sent_start'].fillna(False, inplace=True)\n",
    "    data['sent_start'] = data['sent_start'].astype(int)\n",
    "    data['sent_index'] = data.groupby('original_index')['sent_start'].cumsum()\n",
    "    data['bert_len'] = data['token'].apply(lambda x: len(tokenizer.tokenize(x)))\n",
    "    data = data[data['bert_len']>0]\n",
    "    data['sent_bert_len'] = data.groupby(['original_index','sent_index'])['bert_len'].cumsum()\n",
    "    doc_sent = {}\n",
    "    for doc_idx, group in data.groupby('original_index'):\n",
    "        i,s = 0,0\n",
    "        sent_dict = {}\n",
    "        group.drop_duplicates(subset=['sent_index'], keep='last', inplace=True)\n",
    "        for idx in group.index:\n",
    "            s+=group.loc[idx, 'sent_bert_len']\n",
    "            if s>500:\n",
    "                i+=1\n",
    "                s=group.loc[idx, 'sent_bert_len']\n",
    "                sent_dict[group.loc[idx,'sent_index']]=i\n",
    "            else:\n",
    "                sent_dict[group.loc[idx,'sent_index']]=i\n",
    "        doc_sent[doc_idx]=sent_dict\n",
    "    data['new_sent_index'] = data.apply(lambda x: doc_sent[x['original_index']][x['sent_index']], axis=1)\n",
    "\n",
    "    with open(args.data_dir+'/test.txt', 'w') as f:\n",
    "        for name, group in data.groupby(['original_index','new_sent_index']):\n",
    "            for idx in group.index:\n",
    "                f.write(group.loc[idx, 'token']+'\\n')\n",
    "            f.write('\\n')\n",
    "            \n",
    "    lab_pred_chunks_list, lab_pred_chunks_prob = predict(args, model, tokenizer, labels, pad_token_label_id, mode=\"test\", prefix='test')\n",
    "    \n",
    "    pred = data.groupby(['original_index','new_sent_index'])['token'].apply(list)\n",
    "    pred = pred.reset_index()\n",
    "    pred.columns=['original_index','new_sent_index','tokens']\n",
    "    pred['pred'] = lab_pred_chunks_list\n",
    "    pred['prob'] = lab_pred_chunks_prob\n",
    "    output=[]\n",
    "    for idx in pred.index:\n",
    "        if len(pred.loc[idx, 'pred'])>0:\n",
    "            words = pred.loc[idx,'tokens']\n",
    "            for (l,p) in zip(pred.loc[idx, 'pred'], pred.loc[idx, 'prob']):\n",
    "                output.append([pred.loc[idx, 'original_index'], ' '.join(words[l[1]:l[2]+1]), p, l[0]])\n",
    "\n",
    "    output = pd.DataFrame(output, columns=['original_index','pred','prob','type'])\n",
    "    output.to_csv(f'sap_product_output.csv', mode='a', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-23T10:31:22.774717Z",
     "start_time": "2021-07-23T10:31:22.761651Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_index</th>\n",
       "      <th>pred</th>\n",
       "      <th>prob</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1500</td>\n",
       "      <td>project management</td>\n",
       "      <td>0.999624</td>\n",
       "      <td>FUNC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1500</td>\n",
       "      <td>motivated</td>\n",
       "      <td>0.999616</td>\n",
       "      <td>POWER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1500</td>\n",
       "      <td>Networking</td>\n",
       "      <td>0.999791</td>\n",
       "      <td>POWER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1500</td>\n",
       "      <td>Customer - focused</td>\n",
       "      <td>0.843582</td>\n",
       "      <td>FUNC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1500</td>\n",
       "      <td>Bachelor</td>\n",
       "      <td>0.999798</td>\n",
       "      <td>QUALIFICATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5088</th>\n",
       "      <td>1739</td>\n",
       "      <td>product strategy</td>\n",
       "      <td>0.999688</td>\n",
       "      <td>FUNC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5089</th>\n",
       "      <td>1739</td>\n",
       "      <td>contract management</td>\n",
       "      <td>0.999928</td>\n",
       "      <td>FUNC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5090</th>\n",
       "      <td>1739</td>\n",
       "      <td>organization</td>\n",
       "      <td>0.999923</td>\n",
       "      <td>POWER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5091</th>\n",
       "      <td>1739</td>\n",
       "      <td>Agile</td>\n",
       "      <td>0.999938</td>\n",
       "      <td>TECH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5092</th>\n",
       "      <td>1739</td>\n",
       "      <td>self - motivation</td>\n",
       "      <td>0.999931</td>\n",
       "      <td>POWER</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5093 rows  4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      original_index                 pred      prob           type\n",
       "0               1500   project management  0.999624           FUNC\n",
       "1               1500            motivated  0.999616          POWER\n",
       "2               1500           Networking  0.999791          POWER\n",
       "3               1500   Customer - focused  0.843582           FUNC\n",
       "4               1500             Bachelor  0.999798  QUALIFICATION\n",
       "...              ...                  ...       ...            ...\n",
       "5088            1739     product strategy  0.999688           FUNC\n",
       "5089            1739  contract management  0.999928           FUNC\n",
       "5090            1739         organization  0.999923          POWER\n",
       "5091            1739                Agile  0.999938           TECH\n",
       "5092            1739    self - motivation  0.999931          POWER\n",
       "\n",
       "[5093 rows x 4 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-16T02:00:56.137679Z",
     "start_time": "2021-07-16T02:00:56.135496Z"
    }
   },
   "outputs": [],
   "source": [
    "# output=pd.read_csv('validation_output.csv', header=None)\n",
    "# output.columns=['original_index','pred','prob','type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-16T02:00:56.558072Z",
     "start_time": "2021-07-16T02:00:56.553848Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{8, 42, 46}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(df.index)-set(output['original_index'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-16T02:00:56.979853Z",
     "start_time": "2021-07-16T02:00:56.977448Z"
    }
   },
   "outputs": [],
   "source": [
    "# df=pd.read_csv('validation_jobs.csv', index_col=0)\n",
    "# df=df.reset_index(drop=True)\n",
    "# df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-16T02:44:57.156640Z",
     "start_time": "2021-07-16T02:44:57.144249Z"
    }
   },
   "outputs": [],
   "source": [
    "df['company'] = df['company'].replace('sap2','sap')\n",
    "for i in df.index:\n",
    "    if df.loc[i,'company']=='add':\n",
    "        if df.loc[i,'work_area']=='Software-Design and Development':\n",
    "            df.loc[i,'company']='sap'\n",
    "        elif df.loc[i,'work_area']=='SOFTWARE_ENGINEERING':\n",
    "            df.loc[i,'company']='google'\n",
    "        elif df.loc[i,'work_area']=='TECHNICAL_SOLUTIONS':\n",
    "            df.loc[i,'company']='google'\n",
    "        elif df.loc[i,'work_area']=='Software Development':\n",
    "            df.loc[i,'company']='aws'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-13T08:26:16.915451Z",
     "start_time": "2021-07-13T08:26:16.912965Z"
    }
   },
   "outputs": [],
   "source": [
    "# df['labels'] = df['labels'].map(eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-16T02:44:57.790533Z",
     "start_time": "2021-07-16T02:44:57.786967Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(txt):\n",
    "    return re.sub('[^A-Za-z0-9#+]+', '', str(txt).lower()).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-16T02:45:24.149598Z",
     "start_time": "2021-07-16T02:45:24.135739Z"
    }
   },
   "outputs": [],
   "source": [
    "def custom_metric(trues,preds):\n",
    "    trues = set([clean_text(i) for i in trues])\n",
    "    preds = set([clean_text(i) for i in preds])\n",
    "    print('trues_only: ', trues-preds)\n",
    "    print('preds_only: ', preds-trues)\n",
    "    inter = trues.intersection(preds)\n",
    "#     inter = [i for i in preds if i in trues]\n",
    "    if 'c' in trues and 'c#' in preds:\n",
    "        inter.add('c#')\n",
    "    if 'objectoriented' in trues and 'objectorienteddesign' in preds:\n",
    "        inter.add('objectorienteddesign')\n",
    "    precision = len(inter)/len(preds) if len(preds)>0 else 0\n",
    "    recall = len(inter)/len(trues) if len(trues)>0 else 0\n",
    "    f1 = 2*precision*recall/(precision+recall) if (precision+recall)>0 else 0\n",
    "    return precision, recall, f1, len(trues),len(preds),len(inter), trues-preds, preds-trues, inter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-16T02:45:25.005457Z",
     "start_time": "2021-07-16T02:45:24.682067Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ms\n",
      "trues_only:  set()\n",
      "preds_only:  {'screenunderstanding', 'mobiledevelopment', 'mldevelopment', 'uiautomation', 'screen'}\n",
      "0.2857142857142857 1.0 0.4444444444444445 2 7 2\n",
      "1 sap\n",
      "trues_only:  {'automated'}\n",
      "preds_only:  {'automatedtests', 'testdrivendevelopment'}\n",
      "0.3333333333333333 0.5 0.4 2 3 1\n",
      "2 aws\n",
      "trues_only:  {'it'}\n",
      "preds_only:  set()\n",
      "0 0.0 0 1 0 0\n",
      "3 sap\n",
      "trues_only:  {'ecommerce', 'platformdevelopment', 'migration', 'programing', 'automated', 'highqualitycode'}\n",
      "preds_only:  {'develop', 'designpatterns', 'code', 'writeautomatedunit', 'javaprograming'}\n",
      "0.6428571428571429 0.6 0.6206896551724138 15 14 9\n",
      "4 sap\n",
      "trues_only:  {'database', 'ui'}\n",
      "preds_only:  {'development', 'software', 'programming'}\n",
      "0.25 0.3333333333333333 0.28571428571428575 3 4 1\n",
      "5 aws\n",
      "trues_only:  set()\n",
      "preds_only:  {'softwaredesign'}\n",
      "0.75 1.0 0.8571428571428571 3 4 3\n",
      "6 google\n",
      "trues_only:  {'enterpriseproducts', '3pintegrations'}\n",
      "preds_only:  set()\n",
      "0 0.0 0 2 0 0\n",
      "7 ms\n",
      "trues_only:  set()\n",
      "preds_only:  {'technicalliteracy'}\n",
      "0.8888888888888888 1.0 0.9411764705882353 8 9 8\n",
      "8 sap\n",
      "trues_only:  {'datamanagement'}\n",
      "preds_only:  set()\n",
      "0 0.0 0 1 0 0\n",
      "9 sap\n",
      "trues_only:  set()\n",
      "preds_only:  {'cloudoperations', 'testdrivenmethodology', 'troubleshooting'}\n",
      "0.5714285714285714 1.0 0.7272727272727273 4 7 4\n",
      "10 google\n",
      "trues_only:  {'developingsolutions', 'designtechniques', 'architecting', 'codingstandards', 'c', 'applicationdevelopment', 'developingdistributedsystemsdesign'}\n",
      "preds_only:  {'distributedsystemsdesign', 'c#', 'mobileapplicationdevelopment', 'developing', 'infrastructure'}\n",
      "0.8 0.7272727272727273 0.761904761904762 22 20 16\n",
      "11 sap\n",
      "trues_only:  set()\n",
      "preds_only:  set()\n",
      "0 0 0 0 0 0\n",
      "12 ms\n",
      "trues_only:  set()\n",
      "preds_only:  {'excel', 'powershell'}\n",
      "0.3333333333333333 1.0 0.5 1 3 1\n",
      "13 sap\n",
      "trues_only:  {'c', 'browserextensions', 'developing', 'objectoriented'}\n",
      "preds_only:  {'softwaredevelopment', 'development', 'c#', 'objectorienteddesignprinciples', 'developingbrowserextensions'}\n",
      "0.7142857142857143 0.7692307692307693 0.7407407407407408 13 14 10\n",
      "14 ms\n",
      "trues_only:  set()\n",
      "preds_only:  set()\n",
      "1.0 1.0 1.0 1 1 1\n",
      "15 google\n",
      "trues_only:  {'retailtechnologies', 'multitenantcloud', 'storagesystems'}\n",
      "preds_only:  {'infrastructure'}\n",
      "0.9333333333333333 0.8235294117647058 0.8749999999999999 17 15 14\n",
      "16 aws\n",
      "trues_only:  {'database'}\n",
      "preds_only:  {'serviceoriented', 'operatinglargescalesystems', 'databaseengine', 'distributeddatabase', 'scalable'}\n",
      "0.5833333333333334 0.875 0.7000000000000001 8 12 7\n",
      "17 ms\n",
      "trues_only:  {'automatedhardwaretest'}\n",
      "preds_only:  {'softwaredevelopment', 'continuousintegration', 'hardwaredevelopment', 'hardwaretest'}\n",
      "0.3333333333333333 0.6666666666666666 0.4444444444444444 3 6 2\n",
      "18 ms\n",
      "trues_only:  set()\n",
      "preds_only:  set()\n",
      "0 0 0 0 0 0\n",
      "19 ms\n",
      "trues_only:  {'microsoftcloud'}\n",
      "preds_only:  {'softwaredevelopment', 'distributedservices'}\n",
      "0.3333333333333333 0.5 0.4 2 3 1\n",
      "20 ms\n",
      "trues_only:  set()\n",
      "preds_only:  {'design', 'agile'}\n",
      "0.7777777777777778 1.0 0.8750000000000001 7 9 7\n",
      "21 google\n",
      "trues_only:  set()\n",
      "preds_only:  set()\n",
      "0 0 0 0 0 0\n",
      "22 sap\n",
      "trues_only:  {'developmentsoftware'}\n",
      "preds_only:  set()\n",
      "0 0.0 0 1 0 0\n",
      "23 sap\n",
      "trues_only:  {'softwaredevelopment', 'scrum'}\n",
      "preds_only:  {'odata', 'softwaredevelopmentcycle'}\n",
      "0.6 0.6 0.6 5 5 3\n",
      "24 sap\n",
      "trues_only:  {'mobilearchitectures', 'technology', 'mobiletechnologies', 'agiledevelopment', 'nativeandroidmobilearchitectures'}\n",
      "preds_only:  {'androidmobilearchitectures', 'userexperience', 'agile'}\n",
      "0.88 0.8148148148148148 0.8461538461538461 27 25 22\n",
      "25 aws\n",
      "trues_only:  set()\n",
      "preds_only:  set()\n",
      "1.0 1.0 1.0 1 1 1\n",
      "26 ms\n",
      "trues_only:  set()\n",
      "preds_only:  {'3ddesign', 'gamedesign', 'augmented'}\n",
      "0.4 1.0 0.5714285714285715 2 5 2\n",
      "27 sap\n",
      "trues_only:  {'clouddevelopment', 'dataarchitectures'}\n",
      "preds_only:  {'multiclouddevelopment', 'designingcomplexdataarchitectures', 'cloudservices'}\n",
      "0.5 0.6 0.5454545454545454 5 6 3\n",
      "28 ms\n",
      "trues_only:  set()\n",
      "preds_only:  {'engineering'}\n",
      "0.6666666666666666 1.0 0.8 2 3 2\n",
      "29 google\n",
      "trues_only:  {'deploying', 'developingapplications'}\n",
      "preds_only:  set()\n",
      "1.0 0.6666666666666666 0.8 6 4 4\n",
      "30 sap\n",
      "trues_only:  {'cloud'}\n",
      "preds_only:  {'cloudnativedevelopment', 'softwaredesignmethodologies'}\n",
      "0.6666666666666666 0.8 0.7272727272727272 5 6 4\n",
      "31 google\n",
      "trues_only:  {'sleuthkittsk', 'develop', 'guidanceencase', 'digitalforensics', 'threatlandscape', 'plasolog2timeline', 'automation', 'xways', 'forensics'}\n",
      "preds_only:  {'sleuth', 'guidance', 'plaso', 'mandiant', 'cellebrite', 'xwaysforensics'}\n",
      "0.76 0.6785714285714286 0.7169811320754718 28 25 19\n",
      "32 sap\n",
      "trues_only:  {'designing', 'testing', 'systemsunix', 'supportproductionsupport', 'innovativesoftware', 'buildingsoftware', 'mongodb', 'maintenance', 'opensourcesoftware', 'maintaining', 'building', 'businessprocessorchestration', 'objectorientedlanguages'}\n",
      "preds_only:  {'mongodbetc', 'unix', 'objectoriented', 'cloudsoftware', 'operationssystems', 'opensource'}\n",
      "0.7931034482758621 0.6388888888888888 0.7076923076923076 36 29 23\n",
      "33 sap\n",
      "trues_only:  {'monitoringprocesses'}\n",
      "preds_only:  {'devops', 'lean'}\n",
      "0.6666666666666666 0.8 0.7272727272727272 5 6 4\n",
      "34 sap\n",
      "trues_only:  {'containers', 'hldhighleveldesign', 'lldlowleveldesign', 'adapterdevelopement'}\n",
      "preds_only:  {'hld', 'cloudservice', 'messagingsystems', 'designingsystems', 'alicloud', 'systemquality', 'operations', 'scrum', 'design', 'lld', 'support', 'agilesoftwaredevelopment', 'integration'}\n",
      "0.5806451612903226 0.8181818181818182 0.679245283018868 22 31 18\n",
      "35 aws\n",
      "trues_only:  {'objectoriented'}\n",
      "preds_only:  {'objectorienteddesign'}\n",
      "1.0 1.0 1.0 11 11 11\n",
      "36 ms\n",
      "trues_only:  set()\n",
      "preds_only:  {'implementing'}\n",
      "0.6666666666666666 1.0 0.8 2 3 2\n",
      "37 aws\n",
      "trues_only:  {'aws'}\n",
      "preds_only:  {'awsservices', 'agile'}\n",
      "0.8461538461538461 0.9166666666666666 0.8799999999999999 12 13 11\n",
      "38 ms\n",
      "trues_only:  {'develop', 'azuresqldatabaseservice'}\n",
      "preds_only:  {'btrees', 'azuresql', 'heaps', 'azuresqldatabase', 'microsoftsql'}\n",
      "0.2857142857142857 0.5 0.36363636363636365 4 7 2\n",
      "39 sap\n",
      "trues_only:  {'cicdpipeline', 'implementation', 'cloudinfrastructure', 'landscapedeployment', 'prometheus', 'design', 'hyperscalers'}\n",
      "preds_only:  {'cloudnative'}\n",
      "0.9230769230769231 0.631578947368421 0.7499999999999999 19 13 12\n",
      "40 google\n",
      "trues_only:  {'technicaldeepdivesintocode', 'distributed'}\n",
      "preds_only:  {'storage', 'performance', 'code', 'distributedsystems'}\n",
      "0.7647058823529411 0.8666666666666667 0.8125 15 17 13\n",
      "41 ms\n",
      "trues_only:  {'technical'}\n",
      "preds_only:  set()\n",
      "0 0.0 0 1 0 0\n",
      "42 ms\n",
      "trues_only:  {'microsoftcloud'}\n",
      "preds_only:  set()\n",
      "0 0.0 0 1 0 0\n",
      "43 aws\n",
      "trues_only:  {'designing', 'objectoriented', 'c', 'authentication', 'authorization', 'microservice'}\n",
      "preds_only:  {'c#', 'microservicearchitectures', 'objectorienteddesign', 'networking'}\n",
      "0.8666666666666667 0.7647058823529411 0.8125 17 15 13\n",
      "44 sap\n",
      "trues_only:  {'azurepubliccloudsolutions', 'cisco', 'ssl', 'automation', 'azurecertifiedadministrator', 'azurearm', 'fortigate'}\n",
      "preds_only:  {'sslvpn', 'networks'}\n",
      "0.9259259259259259 0.78125 0.847457627118644 32 27 25\n",
      "45 sap\n",
      "trues_only:  {'docker', 'cicd', 'slareporting', 'embeddingsecurityprocess', 'deploymentmanagementsystem', 'publiccloudproviders', 'developtools'}\n",
      "preds_only:  {'fastpaced', 'continuousintegration', 'dockerautomation', 'operations', 'automated', 'cd', 'deploymentmanagement', 'configuration', 'ci'}\n",
      "0.7272727272727273 0.7741935483870968 0.7500000000000001 31 33 24\n",
      "46 sap\n",
      "trues_only:  set()\n",
      "preds_only:  set()\n",
      "0 0 0 0 0 0\n",
      "47 sap\n",
      "trues_only:  set()\n",
      "preds_only:  {'agile'}\n",
      "0.0 0 0 0 1 0\n",
      "48 ms\n",
      "trues_only:  set()\n",
      "preds_only:  set()\n",
      "1.0 1.0 1.0 4 4 4\n",
      "49 aws\n",
      "trues_only:  {'apis', 'objectoriented'}\n",
      "preds_only:  {'objectorienteddesign', 'backendapis', 'software', 'frontenddesigns', 'objectorientedprogramming'}\n",
      "0.7333333333333333 0.9166666666666666 0.8148148148148148 12 15 11\n",
      "50 ms\n",
      "trues_only:  {'microsoftleap'}\n",
      "preds_only:  set()\n",
      "1.0 0.8333333333333334 0.9090909090909091 6 5 5\n"
     ]
    }
   ],
   "source": [
    "for i in df.index:\n",
    "    print(i, df.loc[i,'company'])\n",
    "    temp = output[(output['original_index']==i)&(output['type']=='TECH')]\n",
    "#     temp = output[(output['original_index']==i)&(output['type']=='TECH')&(output['prob']>0.99)]\n",
    "    trues = []\n",
    "    t = df.loc[i,'text']\n",
    "    for item in df.loc[i,'labels']:\n",
    "        if item[-1]=='TECH':\n",
    "            trues.append(t[item[0]:item[1]])\n",
    "    preds = temp['pred'].tolist()\n",
    "    p,r,f1,n_trues,n_preds,n_inter,trues_only,preds_only,inter = custom_metric(trues,preds)\n",
    "    df.loc[i,'precision']=p\n",
    "    df.loc[i,'recall']=r\n",
    "    df.loc[i,'f1']=f1\n",
    "    df.loc[i,'n_trues']=n_trues\n",
    "    df.loc[i,'n_preds']=n_preds\n",
    "    df.loc[i,'n_inter']=n_inter\n",
    "    df.loc[i,'trues_only']=' , '.join(trues_only)\n",
    "    df.loc[i,'preds_only']=' , '.join(preds_only)\n",
    "    df.loc[i,'inter']=' , '.join(inter)\n",
    "    print(p,r,f1,n_trues,n_preds,n_inter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-16T02:45:26.020230Z",
     "start_time": "2021-07-16T02:45:26.009490Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "precision    0.544789\n",
       "recall       0.631318\n",
       "f1           0.569314\n",
       "dtype: float64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df = df[df['n_trues']>0]\n",
    "df[['precision','recall','f1']].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-16T02:19:28.134214Z",
     "start_time": "2021-07-16T02:19:28.127465Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "precision    0.750696\n",
       "recall       0.777963\n",
       "f1           0.754292\n",
       "dtype: float64"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df['n_trues']>3, ['precision','recall','f1']].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-16T02:19:28.430440Z",
     "start_time": "2021-07-16T02:19:28.423415Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "precision    0.276190\n",
       "recall       0.434783\n",
       "f1           0.326225\n",
       "dtype: float64"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df['n_trues']<=3, ['precision','recall','f1']].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-16T02:45:30.382792Z",
     "start_time": "2021-07-16T02:45:30.371223Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7369614512471655, 0.7611241217798594, 0.748847926267281)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = df['n_inter'].sum()/df['n_preds'].sum()\n",
    "r = df['n_inter'].sum()/df['n_trues'].sum()\n",
    "p, r, 2*p*r/(p+r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-16T02:45:31.524401Z",
     "start_time": "2021-07-16T02:45:31.504903Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aws 8\n",
      "0.8028169014084507 0.8769230769230769 0.8382352941176471\n",
      "google 7\n",
      "0.8148148148148148 0.7333333333333333 0.7719298245614035\n",
      "ms 16\n",
      "0.6 0.8478260869565217 0.7027027027027027\n",
      "sap 20\n",
      "0.7276785714285714 0.7212389380530974 0.7244444444444446\n"
     ]
    }
   ],
   "source": [
    "for name, group in df.groupby(['company']):\n",
    "    print(name, group.shape[0])\n",
    "    p = group['n_inter'].sum()/group['n_preds'].sum()\n",
    "    r = group['n_inter'].sum()/group['n_trues'].sum()\n",
    "    print(p, r, 2*p*r/(p+r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-16T02:20:52.162590Z",
     "start_time": "2021-07-16T02:20:52.149623Z"
    }
   },
   "outputs": [],
   "source": [
    "df.to_csv('./evaluate/ner_evaluation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-12T14:15:30.315422Z",
     "start_time": "2021-07-12T14:15:30.295364Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-13T08:20:44.061405Z",
     "start_time": "2021-07-13T08:20:01.678Z"
    }
   },
   "outputs": [],
   "source": [
    "# pred = data.groupby(['original_index','new_sent_index']).agg({'token':list, 'start_idx':list, 'end_idx':list})\n",
    "# pred = pred.reset_index()\n",
    "# pred.columns=['original_index','new_sent_index','tokens','start_idx','end_idx']\n",
    "# pred['pred'] = lab_pred_chunks_list\n",
    "# pred['prob'] = lab_pred_chunks_prob\n",
    "# output=[]\n",
    "# for idx in pred.index:\n",
    "#     if len(pred.loc[idx, 'pred'])>0:\n",
    "#         for (l,p) in zip(pred.loc[idx, 'pred'], pred.loc[idx, 'prob']):\n",
    "#             output.append([pred.loc[idx, 'original_index'], [pred.loc[idx, 'start_idx'][l[1]], pred.loc[idx, 'end_idx'][l[2]], l[0]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-13T08:20:44.062872Z",
     "start_time": "2021-07-13T08:20:01.680Z"
    }
   },
   "outputs": [],
   "source": [
    "# output=pd.DataFrame(output, columns=['index','labels'])\n",
    "# output = output.groupby('index')['labels'].apply(list)\n",
    "# df['labels']=output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-13T08:20:44.064587Z",
     "start_time": "2021-07-13T08:20:01.682Z"
    },
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "07c6257a1a684a70a5f908bfed85e151",
      "567f5318a39045b4a631336251074b0c",
      "730a101e0d934ee1a8d6f59fdc5591af",
      "23ca656200bc44d2ad3a3f54ccad23b0",
      "e3654e211b554a429a4bd682091baef7",
      "53fc50e2dd144370b2949a07960a3639",
      "cfe65173cf22446cb8e7fcbc590cdc4e",
      "3ed5458b2ecc4776bf72aee22ea5b95f",
      "e625426205f94ca09b225f531d2d13ef",
      "48658241ead44816bf7b373473c4eb67",
      "ca51306da2a04486b35cd4e08d7918b6",
      "f21abba49c7a41ee912293699be72372",
      "79f9c1ebbd804fc78c902eb402f924e6",
      "ac364ae850714cefad6f05d90fc2bad2",
      "4ef5fb14f62242908ae17526459081e4",
      "48ece5df98a5432f8cc52c1d468e73bc",
      "81aec320465b42af9e1f574f715b1a48",
      "5eb7244064bb401c972a15cc582bf231",
      "b42ca923967b4152a06e1074c2087690",
      "6bbbf02d28124c7c851355aa3eed87d0",
      "e4918b46010948c78ddcbbff794015e1",
      "ae8427aee6ee44228b88e56b35b49ed4",
      "1896eaa685304690b0caf3556a5e8cd7",
      "0a37fb1556f644628f436af5f1cbf143",
      "b332e35bdc8a4928b90ca1822e2b5d9d",
      "5067513286af423bbe55ac2311951d4a",
      "4983da4302b84d6189af39a1d4a6241d",
      "ef0bc0c72ee446fd8ca54da7f5e2449c",
      "885c959c58e64c36b9a6cb35a86ca611",
      "7351191476ef441f94a2dc6d8f4be40e",
      "f64e33ade7834a89ac1d9be801a51e78",
      "4fb0ab09d7894b44961bee7306e31053",
      "a87cb7a61d6e44af906bf2843b193c06",
      "ae356230c29b4585b2bd07a5b9359c50",
      "e713517e4e8d4140b643d9cd50187b75",
      "da6175ae962444d1ba2621325ecc2a9b",
      "31fa267934ae49849d47a03e2421f8bd",
      "324766d98e7e45ba9c4e7578025a4918",
      "19085fa2804b440a9ab4b648f9244381",
      "e7786e7873d14a9abdf9ea65f9aaf0aa"
     ]
    },
    "id": "J9tKGUDj19jF",
    "outputId": "c691e9b2-0479-44c3-a09b-d38159bd77bb"
   },
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "# with open('google_labels_doccano.jsonl','w') as f:\n",
    "#     for i in df.index:\n",
    "#         j={'text':df.loc[i,'description_raw'], 'meta':{'company':df.loc[i,'company_name'],'work_area':df.loc[i,'work_area']}, 'labels':df.loc[i,'labels']}\n",
    "#         json.dump(j,f)\n",
    "#         f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-30T18:01:12.323907Z",
     "start_time": "2021-06-30T18:01:12.220652Z"
    },
    "id": "o-Kaj4Yce0Pv"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-15T18:14:38.512264Z",
     "start_time": "2021-07-15T18:14:38.507379Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-16T02:42:27.479492Z",
     "start_time": "2021-07-16T02:42:27.470085Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOkJVRwaFd59YEDRfx5Tw0p",
   "collapsed_sections": [],
   "mount_file_id": "1F3Jd06K-4nAQzHc9LW5BcStr6_2sJgju",
   "name": "doccano train all crf.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "07c6257a1a684a70a5f908bfed85e151": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_567f5318a39045b4a631336251074b0c",
       "IPY_MODEL_730a101e0d934ee1a8d6f59fdc5591af"
      ],
      "layout": "IPY_MODEL_23ca656200bc44d2ad3a3f54ccad23b0"
     }
    },
    "0a37fb1556f644628f436af5f1cbf143": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1896eaa685304690b0caf3556a5e8cd7": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "19085fa2804b440a9ab4b648f9244381": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "23ca656200bc44d2ad3a3f54ccad23b0": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "31fa267934ae49849d47a03e2421f8bd": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "324766d98e7e45ba9c4e7578025a4918": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "3ed5458b2ecc4776bf72aee22ea5b95f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "48658241ead44816bf7b373473c4eb67": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_79f9c1ebbd804fc78c902eb402f924e6",
      "max": 231508,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ac364ae850714cefad6f05d90fc2bad2",
      "value": 231508
     }
    },
    "48ece5df98a5432f8cc52c1d468e73bc": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4983da4302b84d6189af39a1d4a6241d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f64e33ade7834a89ac1d9be801a51e78",
      "placeholder": "",
      "style": "IPY_MODEL_4fb0ab09d7894b44961bee7306e31053",
      "value": " 28.0/28.0 [00:00&lt;00:00, 119B/s]"
     }
    },
    "4ef5fb14f62242908ae17526459081e4": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4fb0ab09d7894b44961bee7306e31053": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5067513286af423bbe55ac2311951d4a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_885c959c58e64c36b9a6cb35a86ca611",
      "max": 28,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_7351191476ef441f94a2dc6d8f4be40e",
      "value": 28
     }
    },
    "53fc50e2dd144370b2949a07960a3639": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "567f5318a39045b4a631336251074b0c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e3654e211b554a429a4bd682091baef7",
      "max": 433,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_53fc50e2dd144370b2949a07960a3639",
      "value": 433
     }
    },
    "5eb7244064bb401c972a15cc582bf231": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e4918b46010948c78ddcbbff794015e1",
      "max": 466062,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ae8427aee6ee44228b88e56b35b49ed4",
      "value": 466062
     }
    },
    "6bbbf02d28124c7c851355aa3eed87d0": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "730a101e0d934ee1a8d6f59fdc5591af": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cfe65173cf22446cb8e7fcbc590cdc4e",
      "placeholder": "",
      "style": "IPY_MODEL_3ed5458b2ecc4776bf72aee22ea5b95f",
      "value": " 433/433 [00:09&lt;00:00, 45.6B/s]"
     }
    },
    "7351191476ef441f94a2dc6d8f4be40e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "79f9c1ebbd804fc78c902eb402f924e6": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "81aec320465b42af9e1f574f715b1a48": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5eb7244064bb401c972a15cc582bf231",
       "IPY_MODEL_b42ca923967b4152a06e1074c2087690"
      ],
      "layout": "IPY_MODEL_6bbbf02d28124c7c851355aa3eed87d0"
     }
    },
    "885c959c58e64c36b9a6cb35a86ca611": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a87cb7a61d6e44af906bf2843b193c06": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ae356230c29b4585b2bd07a5b9359c50",
       "IPY_MODEL_e713517e4e8d4140b643d9cd50187b75"
      ],
      "layout": "IPY_MODEL_da6175ae962444d1ba2621325ecc2a9b"
     }
    },
    "ac364ae850714cefad6f05d90fc2bad2": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "ae356230c29b4585b2bd07a5b9359c50": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_31fa267934ae49849d47a03e2421f8bd",
      "max": 440473133,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_324766d98e7e45ba9c4e7578025a4918",
      "value": 440473133
     }
    },
    "ae8427aee6ee44228b88e56b35b49ed4": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "b332e35bdc8a4928b90ca1822e2b5d9d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5067513286af423bbe55ac2311951d4a",
       "IPY_MODEL_4983da4302b84d6189af39a1d4a6241d"
      ],
      "layout": "IPY_MODEL_ef0bc0c72ee446fd8ca54da7f5e2449c"
     }
    },
    "b42ca923967b4152a06e1074c2087690": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1896eaa685304690b0caf3556a5e8cd7",
      "placeholder": "",
      "style": "IPY_MODEL_0a37fb1556f644628f436af5f1cbf143",
      "value": " 466k/466k [00:00&lt;00:00, 1.22MB/s]"
     }
    },
    "ca51306da2a04486b35cd4e08d7918b6": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4ef5fb14f62242908ae17526459081e4",
      "placeholder": "",
      "style": "IPY_MODEL_48ece5df98a5432f8cc52c1d468e73bc",
      "value": " 232k/232k [00:09&lt;00:00, 24.7kB/s]"
     }
    },
    "cfe65173cf22446cb8e7fcbc590cdc4e": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "da6175ae962444d1ba2621325ecc2a9b": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e3654e211b554a429a4bd682091baef7": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e4918b46010948c78ddcbbff794015e1": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e625426205f94ca09b225f531d2d13ef": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_48658241ead44816bf7b373473c4eb67",
       "IPY_MODEL_ca51306da2a04486b35cd4e08d7918b6"
      ],
      "layout": "IPY_MODEL_f21abba49c7a41ee912293699be72372"
     }
    },
    "e713517e4e8d4140b643d9cd50187b75": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_19085fa2804b440a9ab4b648f9244381",
      "placeholder": "",
      "style": "IPY_MODEL_e7786e7873d14a9abdf9ea65f9aaf0aa",
      "value": " 440M/440M [00:08&lt;00:00, 49.5MB/s]"
     }
    },
    "e7786e7873d14a9abdf9ea65f9aaf0aa": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ef0bc0c72ee446fd8ca54da7f5e2449c": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f21abba49c7a41ee912293699be72372": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f64e33ade7834a89ac1d9be801a51e78": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
